{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST by Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-05T13:43:00.345003Z",
     "iopub.status.busy": "2021-01-05T13:43:00.345003Z",
     "iopub.status.idle": "2021-01-05T13:45:58.241886Z",
     "shell.execute_reply": "2021-01-05T13:45:58.241886Z",
     "shell.execute_reply.started": "2021-01-05T13:43:00.345003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training log: 1 epoch (128 / 60000 train. data). Loss: 2.4106574058532715\n",
      "Training log: 1 epoch (1408 / 60000 train. data). Loss: 1.8252040147781372\n",
      "Training log: 1 epoch (2688 / 60000 train. data). Loss: 1.3773432970046997\n",
      "Training log: 1 epoch (3968 / 60000 train. data). Loss: 1.1139030456542969\n",
      "Training log: 1 epoch (5248 / 60000 train. data). Loss: 0.9657837748527527\n",
      "Training log: 1 epoch (6528 / 60000 train. data). Loss: 0.7119368314743042\n",
      "Training log: 1 epoch (7808 / 60000 train. data). Loss: 0.6182394623756409\n",
      "Training log: 1 epoch (9088 / 60000 train. data). Loss: 0.6987106204032898\n",
      "Training log: 1 epoch (10368 / 60000 train. data). Loss: 0.5788244009017944\n",
      "Training log: 1 epoch (11648 / 60000 train. data). Loss: 0.6022712588310242\n",
      "Training log: 1 epoch (12928 / 60000 train. data). Loss: 0.470750629901886\n",
      "Training log: 1 epoch (14208 / 60000 train. data). Loss: 0.4606071710586548\n",
      "Training log: 1 epoch (15488 / 60000 train. data). Loss: 0.3643248975276947\n",
      "Training log: 1 epoch (16768 / 60000 train. data). Loss: 0.37773314118385315\n",
      "Training log: 1 epoch (18048 / 60000 train. data). Loss: 0.37945497035980225\n",
      "Training log: 1 epoch (19328 / 60000 train. data). Loss: 0.36118239164352417\n",
      "Training log: 1 epoch (20608 / 60000 train. data). Loss: 0.5315171480178833\n",
      "Training log: 1 epoch (21888 / 60000 train. data). Loss: 0.3143908381462097\n",
      "Training log: 1 epoch (23168 / 60000 train. data). Loss: 0.26772040128707886\n",
      "Training log: 1 epoch (24448 / 60000 train. data). Loss: 0.2474609762430191\n",
      "Training log: 1 epoch (25728 / 60000 train. data). Loss: 0.4329059422016144\n",
      "Training log: 1 epoch (27008 / 60000 train. data). Loss: 0.23056720197200775\n",
      "Training log: 1 epoch (28288 / 60000 train. data). Loss: 0.4470371901988983\n",
      "Training log: 1 epoch (29568 / 60000 train. data). Loss: 0.2573944926261902\n",
      "Training log: 1 epoch (30848 / 60000 train. data). Loss: 0.321774959564209\n",
      "Training log: 1 epoch (32128 / 60000 train. data). Loss: 0.3285433053970337\n",
      "Training log: 1 epoch (33408 / 60000 train. data). Loss: 0.23010942339897156\n",
      "Training log: 1 epoch (34688 / 60000 train. data). Loss: 0.22663435339927673\n",
      "Training log: 1 epoch (35968 / 60000 train. data). Loss: 0.22452807426452637\n",
      "Training log: 1 epoch (37248 / 60000 train. data). Loss: 0.3618881404399872\n",
      "Training log: 1 epoch (38528 / 60000 train. data). Loss: 0.1842803657054901\n",
      "Training log: 1 epoch (39808 / 60000 train. data). Loss: 0.2867848575115204\n",
      "Training log: 1 epoch (41088 / 60000 train. data). Loss: 0.18991148471832275\n",
      "Training log: 1 epoch (42368 / 60000 train. data). Loss: 0.27569878101348877\n",
      "Training log: 1 epoch (43648 / 60000 train. data). Loss: 0.37472906708717346\n",
      "Training log: 1 epoch (44928 / 60000 train. data). Loss: 0.33448460698127747\n",
      "Training log: 1 epoch (46208 / 60000 train. data). Loss: 0.34696856141090393\n",
      "Training log: 1 epoch (47488 / 60000 train. data). Loss: 0.3673209547996521\n",
      "Training log: 1 epoch (48768 / 60000 train. data). Loss: 0.23162929713726044\n",
      "Training log: 1 epoch (50048 / 60000 train. data). Loss: 0.2737426161766052\n",
      "Training log: 1 epoch (51328 / 60000 train. data). Loss: 0.31051215529441833\n",
      "Training log: 1 epoch (52608 / 60000 train. data). Loss: 0.25\n",
      "Training log: 1 epoch (53888 / 60000 train. data). Loss: 0.3053709864616394\n",
      "Training log: 1 epoch (55168 / 60000 train. data). Loss: 0.2707981467247009\n",
      "Training log: 1 epoch (56448 / 60000 train. data). Loss: 0.3688254952430725\n",
      "Training log: 1 epoch (57728 / 60000 train. data). Loss: 0.33141788840293884\n",
      "Training log: 1 epoch (59008 / 60000 train. data). Loss: 0.170793816447258\n",
      "Test loss (avg): 0.25729767837524414, Accuracy: 0.9232\n",
      "Training log: 2 epoch (128 / 60000 train. data). Loss: 0.22985777258872986\n",
      "Training log: 2 epoch (1408 / 60000 train. data). Loss: 0.2791286110877991\n",
      "Training log: 2 epoch (2688 / 60000 train. data). Loss: 0.2224343866109848\n",
      "Training log: 2 epoch (3968 / 60000 train. data). Loss: 0.41898882389068604\n",
      "Training log: 2 epoch (5248 / 60000 train. data). Loss: 0.2869676649570465\n",
      "Training log: 2 epoch (6528 / 60000 train. data). Loss: 0.4171147048473358\n",
      "Training log: 2 epoch (7808 / 60000 train. data). Loss: 0.20565643906593323\n",
      "Training log: 2 epoch (9088 / 60000 train. data). Loss: 0.2695036232471466\n",
      "Training log: 2 epoch (10368 / 60000 train. data). Loss: 0.17787723243236542\n",
      "Training log: 2 epoch (11648 / 60000 train. data). Loss: 0.35811448097229004\n",
      "Training log: 2 epoch (12928 / 60000 train. data). Loss: 0.18960751593112946\n",
      "Training log: 2 epoch (14208 / 60000 train. data). Loss: 0.28715619444847107\n",
      "Training log: 2 epoch (15488 / 60000 train. data). Loss: 0.306721568107605\n",
      "Training log: 2 epoch (16768 / 60000 train. data). Loss: 0.2451934665441513\n",
      "Training log: 2 epoch (18048 / 60000 train. data). Loss: 0.31550800800323486\n",
      "Training log: 2 epoch (19328 / 60000 train. data). Loss: 0.31206682324409485\n",
      "Training log: 2 epoch (20608 / 60000 train. data). Loss: 0.273898720741272\n",
      "Training log: 2 epoch (21888 / 60000 train. data). Loss: 0.24501480162143707\n",
      "Training log: 2 epoch (23168 / 60000 train. data). Loss: 0.14547432959079742\n",
      "Training log: 2 epoch (24448 / 60000 train. data). Loss: 0.20252855122089386\n",
      "Training log: 2 epoch (25728 / 60000 train. data). Loss: 0.15685181319713593\n",
      "Training log: 2 epoch (27008 / 60000 train. data). Loss: 0.15678028762340546\n",
      "Training log: 2 epoch (28288 / 60000 train. data). Loss: 0.23813281953334808\n",
      "Training log: 2 epoch (29568 / 60000 train. data). Loss: 0.1356448531150818\n",
      "Training log: 2 epoch (30848 / 60000 train. data). Loss: 0.15441705286502838\n",
      "Training log: 2 epoch (32128 / 60000 train. data). Loss: 0.36776721477508545\n",
      "Training log: 2 epoch (33408 / 60000 train. data). Loss: 0.19579248130321503\n",
      "Training log: 2 epoch (34688 / 60000 train. data). Loss: 0.23116596043109894\n",
      "Training log: 2 epoch (35968 / 60000 train. data). Loss: 0.20130319893360138\n",
      "Training log: 2 epoch (37248 / 60000 train. data). Loss: 0.15022902190685272\n",
      "Training log: 2 epoch (38528 / 60000 train. data). Loss: 0.2552671432495117\n",
      "Training log: 2 epoch (39808 / 60000 train. data). Loss: 0.23130172491073608\n",
      "Training log: 2 epoch (41088 / 60000 train. data). Loss: 0.17590424418449402\n",
      "Training log: 2 epoch (42368 / 60000 train. data). Loss: 0.153560608625412\n",
      "Training log: 2 epoch (43648 / 60000 train. data). Loss: 0.2118963599205017\n",
      "Training log: 2 epoch (44928 / 60000 train. data). Loss: 0.15431374311447144\n",
      "Training log: 2 epoch (46208 / 60000 train. data). Loss: 0.12288211286067963\n",
      "Training log: 2 epoch (47488 / 60000 train. data). Loss: 0.21787448227405548\n",
      "Training log: 2 epoch (48768 / 60000 train. data). Loss: 0.20010149478912354\n",
      "Training log: 2 epoch (50048 / 60000 train. data). Loss: 0.1568116396665573\n",
      "Training log: 2 epoch (51328 / 60000 train. data). Loss: 0.13758009672164917\n",
      "Training log: 2 epoch (52608 / 60000 train. data). Loss: 0.31553614139556885\n",
      "Training log: 2 epoch (53888 / 60000 train. data). Loss: 0.21770939230918884\n",
      "Training log: 2 epoch (55168 / 60000 train. data). Loss: 0.30674564838409424\n",
      "Training log: 2 epoch (56448 / 60000 train. data). Loss: 0.13513721525669098\n",
      "Training log: 2 epoch (57728 / 60000 train. data). Loss: 0.17232826352119446\n",
      "Training log: 2 epoch (59008 / 60000 train. data). Loss: 0.3297712504863739\n",
      "Test loss (avg): 0.1997483792990446, Accuracy: 0.9389\n",
      "Training log: 3 epoch (128 / 60000 train. data). Loss: 0.18708689510822296\n",
      "Training log: 3 epoch (1408 / 60000 train. data). Loss: 0.15665563941001892\n",
      "Training log: 3 epoch (2688 / 60000 train. data). Loss: 0.18690341711044312\n",
      "Training log: 3 epoch (3968 / 60000 train. data). Loss: 0.2157060205936432\n",
      "Training log: 3 epoch (5248 / 60000 train. data). Loss: 0.26846352219581604\n",
      "Training log: 3 epoch (6528 / 60000 train. data). Loss: 0.30097755789756775\n",
      "Training log: 3 epoch (7808 / 60000 train. data). Loss: 0.16387295722961426\n",
      "Training log: 3 epoch (9088 / 60000 train. data). Loss: 0.21148681640625\n",
      "Training log: 3 epoch (10368 / 60000 train. data). Loss: 0.156438410282135\n",
      "Training log: 3 epoch (11648 / 60000 train. data). Loss: 0.25346970558166504\n",
      "Training log: 3 epoch (12928 / 60000 train. data). Loss: 0.1649700254201889\n",
      "Training log: 3 epoch (14208 / 60000 train. data). Loss: 0.11432569473981857\n",
      "Training log: 3 epoch (15488 / 60000 train. data). Loss: 0.211899533867836\n",
      "Training log: 3 epoch (16768 / 60000 train. data). Loss: 0.23046542704105377\n",
      "Training log: 3 epoch (18048 / 60000 train. data). Loss: 0.1257510632276535\n",
      "Training log: 3 epoch (19328 / 60000 train. data). Loss: 0.22541502118110657\n",
      "Training log: 3 epoch (20608 / 60000 train. data). Loss: 0.19873127341270447\n",
      "Training log: 3 epoch (21888 / 60000 train. data). Loss: 0.22471874952316284\n",
      "Training log: 3 epoch (23168 / 60000 train. data). Loss: 0.14684203267097473\n",
      "Training log: 3 epoch (24448 / 60000 train. data). Loss: 0.11028893291950226\n",
      "Training log: 3 epoch (25728 / 60000 train. data). Loss: 0.19277708232402802\n",
      "Training log: 3 epoch (27008 / 60000 train. data). Loss: 0.244881272315979\n",
      "Training log: 3 epoch (28288 / 60000 train. data). Loss: 0.27604106068611145\n",
      "Training log: 3 epoch (29568 / 60000 train. data). Loss: 0.1733139008283615\n",
      "Training log: 3 epoch (30848 / 60000 train. data). Loss: 0.24031095206737518\n",
      "Training log: 3 epoch (32128 / 60000 train. data). Loss: 0.21098488569259644\n",
      "Training log: 3 epoch (33408 / 60000 train. data). Loss: 0.08059553802013397\n",
      "Training log: 3 epoch (34688 / 60000 train. data). Loss: 0.1673755645751953\n",
      "Training log: 3 epoch (35968 / 60000 train. data). Loss: 0.26704004406929016\n",
      "Training log: 3 epoch (37248 / 60000 train. data). Loss: 0.19589486718177795\n",
      "Training log: 3 epoch (38528 / 60000 train. data). Loss: 0.19591236114501953\n",
      "Training log: 3 epoch (39808 / 60000 train. data). Loss: 0.1563330888748169\n",
      "Training log: 3 epoch (41088 / 60000 train. data). Loss: 0.08455275744199753\n",
      "Training log: 3 epoch (42368 / 60000 train. data). Loss: 0.11245817691087723\n",
      "Training log: 3 epoch (43648 / 60000 train. data). Loss: 0.14838580787181854\n",
      "Training log: 3 epoch (44928 / 60000 train. data). Loss: 0.16604292392730713\n",
      "Training log: 3 epoch (46208 / 60000 train. data). Loss: 0.10815871506929398\n",
      "Training log: 3 epoch (47488 / 60000 train. data). Loss: 0.12847372889518738\n",
      "Training log: 3 epoch (48768 / 60000 train. data). Loss: 0.15931281447410583\n",
      "Training log: 3 epoch (50048 / 60000 train. data). Loss: 0.2066890150308609\n",
      "Training log: 3 epoch (51328 / 60000 train. data). Loss: 0.10542226582765579\n",
      "Training log: 3 epoch (52608 / 60000 train. data). Loss: 0.11149052530527115\n",
      "Training log: 3 epoch (53888 / 60000 train. data). Loss: 0.03673085942864418\n",
      "Training log: 3 epoch (55168 / 60000 train. data). Loss: 0.20646362006664276\n",
      "Training log: 3 epoch (56448 / 60000 train. data). Loss: 0.1981530636548996\n",
      "Training log: 3 epoch (57728 / 60000 train. data). Loss: 0.16558478772640228\n",
      "Training log: 3 epoch (59008 / 60000 train. data). Loss: 0.18756499886512756\n",
      "Test loss (avg): 0.15971785962581633, Accuracy: 0.9522\n",
      "Training log: 4 epoch (128 / 60000 train. data). Loss: 0.11190230399370193\n",
      "Training log: 4 epoch (1408 / 60000 train. data). Loss: 0.16959857940673828\n",
      "Training log: 4 epoch (2688 / 60000 train. data). Loss: 0.18908646702766418\n",
      "Training log: 4 epoch (3968 / 60000 train. data). Loss: 0.15083126723766327\n",
      "Training log: 4 epoch (5248 / 60000 train. data). Loss: 0.20519746840000153\n",
      "Training log: 4 epoch (6528 / 60000 train. data). Loss: 0.10037963092327118\n",
      "Training log: 4 epoch (7808 / 60000 train. data). Loss: 0.139357328414917\n",
      "Training log: 4 epoch (9088 / 60000 train. data). Loss: 0.1154002919793129\n",
      "Training log: 4 epoch (10368 / 60000 train. data). Loss: 0.12329046428203583\n",
      "Training log: 4 epoch (11648 / 60000 train. data). Loss: 0.17653250694274902\n",
      "Training log: 4 epoch (12928 / 60000 train. data). Loss: 0.15237578749656677\n",
      "Training log: 4 epoch (14208 / 60000 train. data). Loss: 0.11180062592029572\n",
      "Training log: 4 epoch (15488 / 60000 train. data). Loss: 0.12247073650360107\n",
      "Training log: 4 epoch (16768 / 60000 train. data). Loss: 0.13814540207386017\n",
      "Training log: 4 epoch (18048 / 60000 train. data). Loss: 0.10237834602594376\n",
      "Training log: 4 epoch (19328 / 60000 train. data). Loss: 0.1676367223262787\n",
      "Training log: 4 epoch (20608 / 60000 train. data). Loss: 0.08961069583892822\n",
      "Training log: 4 epoch (21888 / 60000 train. data). Loss: 0.12910096347332\n",
      "Training log: 4 epoch (23168 / 60000 train. data). Loss: 0.09542848914861679\n",
      "Training log: 4 epoch (24448 / 60000 train. data). Loss: 0.1652013063430786\n",
      "Training log: 4 epoch (25728 / 60000 train. data). Loss: 0.1411695033311844\n",
      "Training log: 4 epoch (27008 / 60000 train. data). Loss: 0.20625968277454376\n",
      "Training log: 4 epoch (28288 / 60000 train. data). Loss: 0.12568296492099762\n",
      "Training log: 4 epoch (29568 / 60000 train. data). Loss: 0.15721474587917328\n",
      "Training log: 4 epoch (30848 / 60000 train. data). Loss: 0.07520320266485214\n",
      "Training log: 4 epoch (32128 / 60000 train. data). Loss: 0.12810879945755005\n",
      "Training log: 4 epoch (33408 / 60000 train. data). Loss: 0.06354468315839767\n",
      "Training log: 4 epoch (34688 / 60000 train. data). Loss: 0.11380389332771301\n",
      "Training log: 4 epoch (35968 / 60000 train. data). Loss: 0.1359957456588745\n",
      "Training log: 4 epoch (37248 / 60000 train. data). Loss: 0.09606823325157166\n",
      "Training log: 4 epoch (38528 / 60000 train. data). Loss: 0.19602294266223907\n",
      "Training log: 4 epoch (39808 / 60000 train. data). Loss: 0.12402577698230743\n",
      "Training log: 4 epoch (41088 / 60000 train. data). Loss: 0.13280825316905975\n",
      "Training log: 4 epoch (42368 / 60000 train. data). Loss: 0.16190235316753387\n",
      "Training log: 4 epoch (43648 / 60000 train. data). Loss: 0.08283589780330658\n",
      "Training log: 4 epoch (44928 / 60000 train. data). Loss: 0.10568519681692123\n",
      "Training log: 4 epoch (46208 / 60000 train. data). Loss: 0.08715980499982834\n",
      "Training log: 4 epoch (47488 / 60000 train. data). Loss: 0.13806042075157166\n",
      "Training log: 4 epoch (48768 / 60000 train. data). Loss: 0.17750269174575806\n",
      "Training log: 4 epoch (50048 / 60000 train. data). Loss: 0.05156540498137474\n",
      "Training log: 4 epoch (51328 / 60000 train. data). Loss: 0.18095995485782623\n",
      "Training log: 4 epoch (52608 / 60000 train. data). Loss: 0.10173656046390533\n",
      "Training log: 4 epoch (53888 / 60000 train. data). Loss: 0.1300312727689743\n",
      "Training log: 4 epoch (55168 / 60000 train. data). Loss: 0.20892612636089325\n",
      "Training log: 4 epoch (56448 / 60000 train. data). Loss: 0.25013402104377747\n",
      "Training log: 4 epoch (57728 / 60000 train. data). Loss: 0.06117215007543564\n",
      "Training log: 4 epoch (59008 / 60000 train. data). Loss: 0.16047221422195435\n",
      "Test loss (avg): 0.13164567719399928, Accuracy: 0.9613\n",
      "Training log: 5 epoch (128 / 60000 train. data). Loss: 0.09157178550958633\n",
      "Training log: 5 epoch (1408 / 60000 train. data). Loss: 0.08751747757196426\n",
      "Training log: 5 epoch (2688 / 60000 train. data). Loss: 0.13759683072566986\n",
      "Training log: 5 epoch (3968 / 60000 train. data). Loss: 0.14738845825195312\n",
      "Training log: 5 epoch (5248 / 60000 train. data). Loss: 0.059647079557180405\n",
      "Training log: 5 epoch (6528 / 60000 train. data). Loss: 0.1107608899474144\n",
      "Training log: 5 epoch (7808 / 60000 train. data). Loss: 0.0537051223218441\n",
      "Training log: 5 epoch (9088 / 60000 train. data). Loss: 0.06727059185504913\n",
      "Training log: 5 epoch (10368 / 60000 train. data). Loss: 0.06321655213832855\n",
      "Training log: 5 epoch (11648 / 60000 train. data). Loss: 0.08194807916879654\n",
      "Training log: 5 epoch (12928 / 60000 train. data). Loss: 0.05008470639586449\n",
      "Training log: 5 epoch (14208 / 60000 train. data). Loss: 0.05662589520215988\n",
      "Training log: 5 epoch (15488 / 60000 train. data). Loss: 0.21027584373950958\n",
      "Training log: 5 epoch (16768 / 60000 train. data). Loss: 0.0712672770023346\n",
      "Training log: 5 epoch (18048 / 60000 train. data). Loss: 0.13771973550319672\n",
      "Training log: 5 epoch (19328 / 60000 train. data). Loss: 0.05298101156949997\n",
      "Training log: 5 epoch (20608 / 60000 train. data). Loss: 0.07650773972272873\n",
      "Training log: 5 epoch (21888 / 60000 train. data). Loss: 0.14915409684181213\n",
      "Training log: 5 epoch (23168 / 60000 train. data). Loss: 0.07817655801773071\n",
      "Training log: 5 epoch (24448 / 60000 train. data). Loss: 0.10121235996484756\n",
      "Training log: 5 epoch (25728 / 60000 train. data). Loss: 0.059733979403972626\n",
      "Training log: 5 epoch (27008 / 60000 train. data). Loss: 0.08300010114908218\n",
      "Training log: 5 epoch (28288 / 60000 train. data). Loss: 0.10797961056232452\n",
      "Training log: 5 epoch (29568 / 60000 train. data). Loss: 0.08410502970218658\n",
      "Training log: 5 epoch (30848 / 60000 train. data). Loss: 0.10960757732391357\n",
      "Training log: 5 epoch (32128 / 60000 train. data). Loss: 0.13798198103904724\n",
      "Training log: 5 epoch (33408 / 60000 train. data). Loss: 0.16645751893520355\n",
      "Training log: 5 epoch (34688 / 60000 train. data). Loss: 0.07285638898611069\n",
      "Training log: 5 epoch (35968 / 60000 train. data). Loss: 0.09488966315984726\n",
      "Training log: 5 epoch (37248 / 60000 train. data). Loss: 0.2465091496706009\n",
      "Training log: 5 epoch (38528 / 60000 train. data). Loss: 0.09747370332479477\n",
      "Training log: 5 epoch (39808 / 60000 train. data). Loss: 0.13947893679141998\n",
      "Training log: 5 epoch (41088 / 60000 train. data). Loss: 0.1145634800195694\n",
      "Training log: 5 epoch (42368 / 60000 train. data). Loss: 0.14464648067951202\n",
      "Training log: 5 epoch (43648 / 60000 train. data). Loss: 0.09284770488739014\n",
      "Training log: 5 epoch (44928 / 60000 train. data). Loss: 0.05611848086118698\n",
      "Training log: 5 epoch (46208 / 60000 train. data). Loss: 0.09999405592679977\n",
      "Training log: 5 epoch (47488 / 60000 train. data). Loss: 0.06457068771123886\n",
      "Training log: 5 epoch (48768 / 60000 train. data). Loss: 0.09366471320390701\n",
      "Training log: 5 epoch (50048 / 60000 train. data). Loss: 0.06816225498914719\n",
      "Training log: 5 epoch (51328 / 60000 train. data). Loss: 0.19817578792572021\n",
      "Training log: 5 epoch (52608 / 60000 train. data). Loss: 0.08295372128486633\n",
      "Training log: 5 epoch (53888 / 60000 train. data). Loss: 0.09406892210245132\n",
      "Training log: 5 epoch (55168 / 60000 train. data). Loss: 0.1128314882516861\n",
      "Training log: 5 epoch (56448 / 60000 train. data). Loss: 0.14116032421588898\n",
      "Training log: 5 epoch (57728 / 60000 train. data). Loss: 0.1029912456870079\n",
      "Training log: 5 epoch (59008 / 60000 train. data). Loss: 0.12761132419109344\n",
      "Test loss (avg): 0.10931385655403138, Accuracy: 0.9665\n",
      "Training log: 6 epoch (128 / 60000 train. data). Loss: 0.17960341274738312\n",
      "Training log: 6 epoch (1408 / 60000 train. data). Loss: 0.07463984191417694\n",
      "Training log: 6 epoch (2688 / 60000 train. data). Loss: 0.04189859703183174\n",
      "Training log: 6 epoch (3968 / 60000 train. data). Loss: 0.09784063696861267\n",
      "Training log: 6 epoch (5248 / 60000 train. data). Loss: 0.19438044726848602\n",
      "Training log: 6 epoch (6528 / 60000 train. data). Loss: 0.06262566894292831\n",
      "Training log: 6 epoch (7808 / 60000 train. data). Loss: 0.09989595413208008\n",
      "Training log: 6 epoch (9088 / 60000 train. data). Loss: 0.08395589143037796\n",
      "Training log: 6 epoch (10368 / 60000 train. data). Loss: 0.08486909419298172\n",
      "Training log: 6 epoch (11648 / 60000 train. data). Loss: 0.11692914366722107\n",
      "Training log: 6 epoch (12928 / 60000 train. data). Loss: 0.11790130287408829\n",
      "Training log: 6 epoch (14208 / 60000 train. data). Loss: 0.0886467695236206\n",
      "Training log: 6 epoch (15488 / 60000 train. data). Loss: 0.05864720046520233\n",
      "Training log: 6 epoch (16768 / 60000 train. data). Loss: 0.05325089022517204\n",
      "Training log: 6 epoch (18048 / 60000 train. data). Loss: 0.05029413476586342\n",
      "Training log: 6 epoch (19328 / 60000 train. data). Loss: 0.15037184953689575\n",
      "Training log: 6 epoch (20608 / 60000 train. data). Loss: 0.08667680621147156\n",
      "Training log: 6 epoch (21888 / 60000 train. data). Loss: 0.10032965242862701\n",
      "Training log: 6 epoch (23168 / 60000 train. data). Loss: 0.08730414509773254\n",
      "Training log: 6 epoch (24448 / 60000 train. data). Loss: 0.06178802624344826\n",
      "Training log: 6 epoch (25728 / 60000 train. data). Loss: 0.06928588449954987\n",
      "Training log: 6 epoch (27008 / 60000 train. data). Loss: 0.11799979954957962\n",
      "Training log: 6 epoch (28288 / 60000 train. data). Loss: 0.11323238164186478\n",
      "Training log: 6 epoch (29568 / 60000 train. data). Loss: 0.05992516875267029\n",
      "Training log: 6 epoch (30848 / 60000 train. data). Loss: 0.12143943458795547\n",
      "Training log: 6 epoch (32128 / 60000 train. data). Loss: 0.11204816401004791\n",
      "Training log: 6 epoch (33408 / 60000 train. data). Loss: 0.06270748376846313\n",
      "Training log: 6 epoch (34688 / 60000 train. data). Loss: 0.1236313134431839\n",
      "Training log: 6 epoch (35968 / 60000 train. data). Loss: 0.042482390999794006\n",
      "Training log: 6 epoch (37248 / 60000 train. data). Loss: 0.12455581873655319\n",
      "Training log: 6 epoch (38528 / 60000 train. data). Loss: 0.06866126507520676\n",
      "Training log: 6 epoch (39808 / 60000 train. data). Loss: 0.08586053550243378\n",
      "Training log: 6 epoch (41088 / 60000 train. data). Loss: 0.04452496021986008\n",
      "Training log: 6 epoch (42368 / 60000 train. data). Loss: 0.055756133049726486\n",
      "Training log: 6 epoch (43648 / 60000 train. data). Loss: 0.09661765396595001\n",
      "Training log: 6 epoch (44928 / 60000 train. data). Loss: 0.05320451408624649\n",
      "Training log: 6 epoch (46208 / 60000 train. data). Loss: 0.05371921509504318\n",
      "Training log: 6 epoch (47488 / 60000 train. data). Loss: 0.07219588756561279\n",
      "Training log: 6 epoch (48768 / 60000 train. data). Loss: 0.0665898323059082\n",
      "Training log: 6 epoch (50048 / 60000 train. data). Loss: 0.13797412812709808\n",
      "Training log: 6 epoch (51328 / 60000 train. data). Loss: 0.0861779972910881\n",
      "Training log: 6 epoch (52608 / 60000 train. data). Loss: 0.10391054302453995\n",
      "Training log: 6 epoch (53888 / 60000 train. data). Loss: 0.09059622883796692\n",
      "Training log: 6 epoch (55168 / 60000 train. data). Loss: 0.06993058323860168\n",
      "Training log: 6 epoch (56448 / 60000 train. data). Loss: 0.09908869862556458\n",
      "Training log: 6 epoch (57728 / 60000 train. data). Loss: 0.10959124565124512\n",
      "Training log: 6 epoch (59008 / 60000 train. data). Loss: 0.154100701212883\n",
      "Test loss (avg): 0.09496908477544784, Accuracy: 0.9716\n",
      "Training log: 7 epoch (128 / 60000 train. data). Loss: 0.02753502130508423\n",
      "Training log: 7 epoch (1408 / 60000 train. data). Loss: 0.11233560740947723\n",
      "Training log: 7 epoch (2688 / 60000 train. data). Loss: 0.07394229620695114\n",
      "Training log: 7 epoch (3968 / 60000 train. data). Loss: 0.08117706328630447\n",
      "Training log: 7 epoch (5248 / 60000 train. data). Loss: 0.06842970103025436\n",
      "Training log: 7 epoch (6528 / 60000 train. data). Loss: 0.062313418835401535\n",
      "Training log: 7 epoch (7808 / 60000 train. data). Loss: 0.05351299047470093\n",
      "Training log: 7 epoch (9088 / 60000 train. data). Loss: 0.1673065721988678\n",
      "Training log: 7 epoch (10368 / 60000 train. data). Loss: 0.15838149189949036\n",
      "Training log: 7 epoch (11648 / 60000 train. data). Loss: 0.08236560970544815\n",
      "Training log: 7 epoch (12928 / 60000 train. data). Loss: 0.05611620470881462\n",
      "Training log: 7 epoch (14208 / 60000 train. data). Loss: 0.09954455494880676\n",
      "Training log: 7 epoch (15488 / 60000 train. data). Loss: 0.0382092148065567\n",
      "Training log: 7 epoch (16768 / 60000 train. data). Loss: 0.069402314722538\n",
      "Training log: 7 epoch (18048 / 60000 train. data). Loss: 0.05407383665442467\n",
      "Training log: 7 epoch (19328 / 60000 train. data). Loss: 0.04468121752142906\n",
      "Training log: 7 epoch (20608 / 60000 train. data). Loss: 0.15709012746810913\n",
      "Training log: 7 epoch (21888 / 60000 train. data). Loss: 0.061910971999168396\n",
      "Training log: 7 epoch (23168 / 60000 train. data). Loss: 0.06978777050971985\n",
      "Training log: 7 epoch (24448 / 60000 train. data). Loss: 0.05455119535326958\n",
      "Training log: 7 epoch (25728 / 60000 train. data). Loss: 0.11231755465269089\n",
      "Training log: 7 epoch (27008 / 60000 train. data). Loss: 0.07555725425481796\n",
      "Training log: 7 epoch (28288 / 60000 train. data). Loss: 0.0719195306301117\n",
      "Training log: 7 epoch (29568 / 60000 train. data). Loss: 0.10498432070016861\n",
      "Training log: 7 epoch (30848 / 60000 train. data). Loss: 0.035819701850414276\n",
      "Training log: 7 epoch (32128 / 60000 train. data). Loss: 0.035710543394088745\n",
      "Training log: 7 epoch (33408 / 60000 train. data). Loss: 0.08210604637861252\n",
      "Training log: 7 epoch (34688 / 60000 train. data). Loss: 0.06291250884532928\n",
      "Training log: 7 epoch (35968 / 60000 train. data). Loss: 0.02416539005935192\n",
      "Training log: 7 epoch (37248 / 60000 train. data). Loss: 0.022885452955961227\n",
      "Training log: 7 epoch (38528 / 60000 train. data). Loss: 0.1238943338394165\n",
      "Training log: 7 epoch (39808 / 60000 train. data). Loss: 0.08885358273983002\n",
      "Training log: 7 epoch (41088 / 60000 train. data). Loss: 0.039793454110622406\n",
      "Training log: 7 epoch (42368 / 60000 train. data). Loss: 0.10120193660259247\n",
      "Training log: 7 epoch (43648 / 60000 train. data). Loss: 0.10428449511528015\n",
      "Training log: 7 epoch (44928 / 60000 train. data). Loss: 0.03850313648581505\n",
      "Training log: 7 epoch (46208 / 60000 train. data). Loss: 0.05229509249329567\n",
      "Training log: 7 epoch (47488 / 60000 train. data). Loss: 0.06664230674505234\n",
      "Training log: 7 epoch (48768 / 60000 train. data). Loss: 0.03575969487428665\n",
      "Training log: 7 epoch (50048 / 60000 train. data). Loss: 0.09786146879196167\n",
      "Training log: 7 epoch (51328 / 60000 train. data). Loss: 0.10305863618850708\n",
      "Training log: 7 epoch (52608 / 60000 train. data). Loss: 0.10957358777523041\n",
      "Training log: 7 epoch (53888 / 60000 train. data). Loss: 0.05089837685227394\n",
      "Training log: 7 epoch (55168 / 60000 train. data). Loss: 0.05141540989279747\n",
      "Training log: 7 epoch (56448 / 60000 train. data). Loss: 0.08761557191610336\n",
      "Training log: 7 epoch (57728 / 60000 train. data). Loss: 0.0572388619184494\n",
      "Training log: 7 epoch (59008 / 60000 train. data). Loss: 0.06991677731275558\n",
      "Test loss (avg): 0.0880565966129303, Accuracy: 0.9725\n",
      "Training log: 8 epoch (128 / 60000 train. data). Loss: 0.07418464869260788\n",
      "Training log: 8 epoch (1408 / 60000 train. data). Loss: 0.023879028856754303\n",
      "Training log: 8 epoch (2688 / 60000 train. data). Loss: 0.08232025802135468\n",
      "Training log: 8 epoch (3968 / 60000 train. data). Loss: 0.07005302608013153\n",
      "Training log: 8 epoch (5248 / 60000 train. data). Loss: 0.03673205524682999\n",
      "Training log: 8 epoch (6528 / 60000 train. data). Loss: 0.11743389070034027\n",
      "Training log: 8 epoch (7808 / 60000 train. data). Loss: 0.019527608528733253\n",
      "Training log: 8 epoch (9088 / 60000 train. data). Loss: 0.058840468525886536\n",
      "Training log: 8 epoch (10368 / 60000 train. data). Loss: 0.07255189120769501\n",
      "Training log: 8 epoch (11648 / 60000 train. data). Loss: 0.07278209924697876\n",
      "Training log: 8 epoch (12928 / 60000 train. data). Loss: 0.0483480729162693\n",
      "Training log: 8 epoch (14208 / 60000 train. data). Loss: 0.043552596122026443\n",
      "Training log: 8 epoch (15488 / 60000 train. data). Loss: 0.11019130796194077\n",
      "Training log: 8 epoch (16768 / 60000 train. data). Loss: 0.08745867758989334\n",
      "Training log: 8 epoch (18048 / 60000 train. data). Loss: 0.02824411727488041\n",
      "Training log: 8 epoch (19328 / 60000 train. data). Loss: 0.03485293313860893\n",
      "Training log: 8 epoch (20608 / 60000 train. data). Loss: 0.060828886926174164\n",
      "Training log: 8 epoch (21888 / 60000 train. data). Loss: 0.0959019660949707\n",
      "Training log: 8 epoch (23168 / 60000 train. data). Loss: 0.024279484525322914\n",
      "Training log: 8 epoch (24448 / 60000 train. data). Loss: 0.05521572381258011\n",
      "Training log: 8 epoch (25728 / 60000 train. data). Loss: 0.09060996025800705\n",
      "Training log: 8 epoch (27008 / 60000 train. data). Loss: 0.047391973435878754\n",
      "Training log: 8 epoch (28288 / 60000 train. data). Loss: 0.07904281467199326\n",
      "Training log: 8 epoch (29568 / 60000 train. data). Loss: 0.042981553822755814\n",
      "Training log: 8 epoch (30848 / 60000 train. data). Loss: 0.035707566887140274\n",
      "Training log: 8 epoch (32128 / 60000 train. data). Loss: 0.053404562175273895\n",
      "Training log: 8 epoch (33408 / 60000 train. data). Loss: 0.03943789377808571\n",
      "Training log: 8 epoch (34688 / 60000 train. data). Loss: 0.03937869518995285\n",
      "Training log: 8 epoch (35968 / 60000 train. data). Loss: 0.05764522776007652\n",
      "Training log: 8 epoch (37248 / 60000 train. data). Loss: 0.06165561452507973\n",
      "Training log: 8 epoch (38528 / 60000 train. data). Loss: 0.07327689975500107\n",
      "Training log: 8 epoch (39808 / 60000 train. data). Loss: 0.030801555141806602\n",
      "Training log: 8 epoch (41088 / 60000 train. data). Loss: 0.06515638530254364\n",
      "Training log: 8 epoch (42368 / 60000 train. data). Loss: 0.11381316184997559\n",
      "Training log: 8 epoch (43648 / 60000 train. data). Loss: 0.03801587596535683\n",
      "Training log: 8 epoch (44928 / 60000 train. data). Loss: 0.017893332988023758\n",
      "Training log: 8 epoch (46208 / 60000 train. data). Loss: 0.05583273246884346\n",
      "Training log: 8 epoch (47488 / 60000 train. data). Loss: 0.03761279210448265\n",
      "Training log: 8 epoch (48768 / 60000 train. data). Loss: 0.08037181943655014\n",
      "Training log: 8 epoch (50048 / 60000 train. data). Loss: 0.06728886812925339\n",
      "Training log: 8 epoch (51328 / 60000 train. data). Loss: 0.03411691635847092\n",
      "Training log: 8 epoch (52608 / 60000 train. data). Loss: 0.05515255779027939\n",
      "Training log: 8 epoch (53888 / 60000 train. data). Loss: 0.07480674982070923\n",
      "Training log: 8 epoch (55168 / 60000 train. data). Loss: 0.10498040169477463\n",
      "Training log: 8 epoch (56448 / 60000 train. data). Loss: 0.0726635605096817\n",
      "Training log: 8 epoch (57728 / 60000 train. data). Loss: 0.033367548137903214\n",
      "Training log: 8 epoch (59008 / 60000 train. data). Loss: 0.04110635444521904\n",
      "Test loss (avg): 0.07938116746544838, Accuracy: 0.9768\n",
      "Training log: 9 epoch (128 / 60000 train. data). Loss: 0.06478069722652435\n",
      "Training log: 9 epoch (1408 / 60000 train. data). Loss: 0.03150974214076996\n",
      "Training log: 9 epoch (2688 / 60000 train. data). Loss: 0.0920780822634697\n",
      "Training log: 9 epoch (3968 / 60000 train. data). Loss: 0.02977723442018032\n",
      "Training log: 9 epoch (5248 / 60000 train. data). Loss: 0.11659612506628036\n",
      "Training log: 9 epoch (6528 / 60000 train. data). Loss: 0.052051231265068054\n",
      "Training log: 9 epoch (7808 / 60000 train. data). Loss: 0.0393294021487236\n",
      "Training log: 9 epoch (9088 / 60000 train. data). Loss: 0.06265011429786682\n",
      "Training log: 9 epoch (10368 / 60000 train. data). Loss: 0.034861791878938675\n",
      "Training log: 9 epoch (11648 / 60000 train. data). Loss: 0.060569968074560165\n",
      "Training log: 9 epoch (12928 / 60000 train. data). Loss: 0.030964242294430733\n",
      "Training log: 9 epoch (14208 / 60000 train. data). Loss: 0.058908361941576004\n",
      "Training log: 9 epoch (15488 / 60000 train. data). Loss: 0.029144957661628723\n",
      "Training log: 9 epoch (16768 / 60000 train. data). Loss: 0.027813568711280823\n",
      "Training log: 9 epoch (18048 / 60000 train. data). Loss: 0.015162471681833267\n",
      "Training log: 9 epoch (19328 / 60000 train. data). Loss: 0.05388788506388664\n",
      "Training log: 9 epoch (20608 / 60000 train. data). Loss: 0.05521732196211815\n",
      "Training log: 9 epoch (21888 / 60000 train. data). Loss: 0.06733271479606628\n",
      "Training log: 9 epoch (23168 / 60000 train. data). Loss: 0.039482150226831436\n",
      "Training log: 9 epoch (24448 / 60000 train. data). Loss: 0.007244685664772987\n",
      "Training log: 9 epoch (25728 / 60000 train. data). Loss: 0.031204208731651306\n",
      "Training log: 9 epoch (27008 / 60000 train. data). Loss: 0.04887659102678299\n",
      "Training log: 9 epoch (28288 / 60000 train. data). Loss: 0.05539626628160477\n",
      "Training log: 9 epoch (29568 / 60000 train. data). Loss: 0.0592193566262722\n",
      "Training log: 9 epoch (30848 / 60000 train. data). Loss: 0.0594417043030262\n",
      "Training log: 9 epoch (32128 / 60000 train. data). Loss: 0.02628771960735321\n",
      "Training log: 9 epoch (33408 / 60000 train. data). Loss: 0.02535572089254856\n",
      "Training log: 9 epoch (34688 / 60000 train. data). Loss: 0.0428963340818882\n",
      "Training log: 9 epoch (35968 / 60000 train. data). Loss: 0.07695646584033966\n",
      "Training log: 9 epoch (37248 / 60000 train. data). Loss: 0.051956839859485626\n",
      "Training log: 9 epoch (38528 / 60000 train. data). Loss: 0.025706853717565536\n",
      "Training log: 9 epoch (39808 / 60000 train. data). Loss: 0.06862428039312363\n",
      "Training log: 9 epoch (41088 / 60000 train. data). Loss: 0.03267868980765343\n",
      "Training log: 9 epoch (42368 / 60000 train. data). Loss: 0.017927149310708046\n",
      "Training log: 9 epoch (43648 / 60000 train. data). Loss: 0.03627226501703262\n",
      "Training log: 9 epoch (44928 / 60000 train. data). Loss: 0.019417664036154747\n",
      "Training log: 9 epoch (46208 / 60000 train. data). Loss: 0.02452186867594719\n",
      "Training log: 9 epoch (47488 / 60000 train. data). Loss: 0.030257927253842354\n",
      "Training log: 9 epoch (48768 / 60000 train. data). Loss: 0.03466383367776871\n",
      "Training log: 9 epoch (50048 / 60000 train. data). Loss: 0.06910476833581924\n",
      "Training log: 9 epoch (51328 / 60000 train. data). Loss: 0.04983513057231903\n",
      "Training log: 9 epoch (52608 / 60000 train. data). Loss: 0.03754205256700516\n",
      "Training log: 9 epoch (53888 / 60000 train. data). Loss: 0.02536856383085251\n",
      "Training log: 9 epoch (55168 / 60000 train. data). Loss: 0.019632795825600624\n",
      "Training log: 9 epoch (56448 / 60000 train. data). Loss: 0.03611748293042183\n",
      "Training log: 9 epoch (57728 / 60000 train. data). Loss: 0.09788830578327179\n",
      "Training log: 9 epoch (59008 / 60000 train. data). Loss: 0.051425982266664505\n",
      "Test loss (avg): 0.07256627287864685, Accuracy: 0.9774\n",
      "Training log: 10 epoch (128 / 60000 train. data). Loss: 0.03788929060101509\n",
      "Training log: 10 epoch (1408 / 60000 train. data). Loss: 0.06848342716693878\n",
      "Training log: 10 epoch (2688 / 60000 train. data). Loss: 0.031450893729925156\n",
      "Training log: 10 epoch (3968 / 60000 train. data). Loss: 0.024671245366334915\n",
      "Training log: 10 epoch (5248 / 60000 train. data). Loss: 0.05687958747148514\n",
      "Training log: 10 epoch (6528 / 60000 train. data). Loss: 0.041100554168224335\n",
      "Training log: 10 epoch (7808 / 60000 train. data). Loss: 0.06380763649940491\n",
      "Training log: 10 epoch (9088 / 60000 train. data). Loss: 0.02429887466132641\n",
      "Training log: 10 epoch (10368 / 60000 train. data). Loss: 0.0475979708135128\n",
      "Training log: 10 epoch (11648 / 60000 train. data). Loss: 0.04543610289692879\n",
      "Training log: 10 epoch (12928 / 60000 train. data). Loss: 0.06261580437421799\n",
      "Training log: 10 epoch (14208 / 60000 train. data). Loss: 0.11678235232830048\n",
      "Training log: 10 epoch (15488 / 60000 train. data). Loss: 0.05650705471634865\n",
      "Training log: 10 epoch (16768 / 60000 train. data). Loss: 0.04788811504840851\n",
      "Training log: 10 epoch (18048 / 60000 train. data). Loss: 0.0415060892701149\n",
      "Training log: 10 epoch (19328 / 60000 train. data). Loss: 0.025387709960341454\n",
      "Training log: 10 epoch (20608 / 60000 train. data). Loss: 0.02711884118616581\n",
      "Training log: 10 epoch (21888 / 60000 train. data). Loss: 0.03159234672784805\n",
      "Training log: 10 epoch (23168 / 60000 train. data). Loss: 0.035323165357112885\n",
      "Training log: 10 epoch (24448 / 60000 train. data). Loss: 0.04810985177755356\n",
      "Training log: 10 epoch (25728 / 60000 train. data). Loss: 0.02183307334780693\n",
      "Training log: 10 epoch (27008 / 60000 train. data). Loss: 0.029350051656365395\n",
      "Training log: 10 epoch (28288 / 60000 train. data). Loss: 0.03409997746348381\n",
      "Training log: 10 epoch (29568 / 60000 train. data). Loss: 0.04422476142644882\n",
      "Training log: 10 epoch (30848 / 60000 train. data). Loss: 0.06619112193584442\n",
      "Training log: 10 epoch (32128 / 60000 train. data). Loss: 0.03573503717780113\n",
      "Training log: 10 epoch (33408 / 60000 train. data). Loss: 0.0366627462208271\n",
      "Training log: 10 epoch (34688 / 60000 train. data). Loss: 0.021114811301231384\n",
      "Training log: 10 epoch (35968 / 60000 train. data). Loss: 0.08203606307506561\n",
      "Training log: 10 epoch (37248 / 60000 train. data). Loss: 0.02376633696258068\n",
      "Training log: 10 epoch (38528 / 60000 train. data). Loss: 0.025219937786459923\n",
      "Training log: 10 epoch (39808 / 60000 train. data). Loss: 0.009264753200113773\n",
      "Training log: 10 epoch (41088 / 60000 train. data). Loss: 0.04950862005352974\n",
      "Training log: 10 epoch (42368 / 60000 train. data). Loss: 0.07616525143384933\n",
      "Training log: 10 epoch (43648 / 60000 train. data). Loss: 0.03416818380355835\n",
      "Training log: 10 epoch (44928 / 60000 train. data). Loss: 0.04805335775017738\n",
      "Training log: 10 epoch (46208 / 60000 train. data). Loss: 0.05988534167408943\n",
      "Training log: 10 epoch (47488 / 60000 train. data). Loss: 0.03109191730618477\n",
      "Training log: 10 epoch (48768 / 60000 train. data). Loss: 0.036024656146764755\n",
      "Training log: 10 epoch (50048 / 60000 train. data). Loss: 0.042966797947883606\n",
      "Training log: 10 epoch (51328 / 60000 train. data). Loss: 0.13906791806221008\n",
      "Training log: 10 epoch (52608 / 60000 train. data). Loss: 0.04445572569966316\n",
      "Training log: 10 epoch (53888 / 60000 train. data). Loss: 0.010508645325899124\n",
      "Training log: 10 epoch (55168 / 60000 train. data). Loss: 0.044278327375650406\n",
      "Training log: 10 epoch (56448 / 60000 train. data). Loss: 0.01918395236134529\n",
      "Training log: 10 epoch (57728 / 60000 train. data). Loss: 0.019704453647136688\n",
      "Training log: 10 epoch (59008 / 60000 train. data). Loss: 0.01057771872729063\n",
      "Test loss (avg): 0.07424206672310829, Accuracy: 0.9759\n",
      "Training log: 11 epoch (128 / 60000 train. data). Loss: 0.02026326395571232\n",
      "Training log: 11 epoch (1408 / 60000 train. data). Loss: 0.01081742811948061\n",
      "Training log: 11 epoch (2688 / 60000 train. data). Loss: 0.061503175646066666\n",
      "Training log: 11 epoch (3968 / 60000 train. data). Loss: 0.07490496337413788\n",
      "Training log: 11 epoch (5248 / 60000 train. data). Loss: 0.02495567500591278\n",
      "Training log: 11 epoch (6528 / 60000 train. data). Loss: 0.03612873703241348\n",
      "Training log: 11 epoch (7808 / 60000 train. data). Loss: 0.05465909466147423\n",
      "Training log: 11 epoch (9088 / 60000 train. data). Loss: 0.08108560740947723\n",
      "Training log: 11 epoch (10368 / 60000 train. data). Loss: 0.02559990994632244\n",
      "Training log: 11 epoch (11648 / 60000 train. data). Loss: 0.07637863606214523\n",
      "Training log: 11 epoch (12928 / 60000 train. data). Loss: 0.021421920508146286\n",
      "Training log: 11 epoch (14208 / 60000 train. data). Loss: 0.010704773478209972\n",
      "Training log: 11 epoch (15488 / 60000 train. data). Loss: 0.01471236813813448\n",
      "Training log: 11 epoch (16768 / 60000 train. data). Loss: 0.010430337861180305\n",
      "Training log: 11 epoch (18048 / 60000 train. data). Loss: 0.02195335365831852\n",
      "Training log: 11 epoch (19328 / 60000 train. data). Loss: 0.041755352169275284\n",
      "Training log: 11 epoch (20608 / 60000 train. data). Loss: 0.04115286469459534\n",
      "Training log: 11 epoch (21888 / 60000 train. data). Loss: 0.05634855106472969\n",
      "Training log: 11 epoch (23168 / 60000 train. data). Loss: 0.05040730908513069\n",
      "Training log: 11 epoch (24448 / 60000 train. data). Loss: 0.03592754900455475\n",
      "Training log: 11 epoch (25728 / 60000 train. data). Loss: 0.025783222168684006\n",
      "Training log: 11 epoch (27008 / 60000 train. data). Loss: 0.053541939705610275\n",
      "Training log: 11 epoch (28288 / 60000 train. data). Loss: 0.05040360614657402\n",
      "Training log: 11 epoch (29568 / 60000 train. data). Loss: 0.07076210528612137\n",
      "Training log: 11 epoch (30848 / 60000 train. data). Loss: 0.03701793774962425\n",
      "Training log: 11 epoch (32128 / 60000 train. data). Loss: 0.044676702469587326\n",
      "Training log: 11 epoch (33408 / 60000 train. data). Loss: 0.015765978023409843\n",
      "Training log: 11 epoch (34688 / 60000 train. data). Loss: 0.0429137758910656\n",
      "Training log: 11 epoch (35968 / 60000 train. data). Loss: 0.009171257726848125\n",
      "Training log: 11 epoch (37248 / 60000 train. data). Loss: 0.017543984577059746\n",
      "Training log: 11 epoch (38528 / 60000 train. data). Loss: 0.020061740651726723\n",
      "Training log: 11 epoch (39808 / 60000 train. data). Loss: 0.05560559779405594\n",
      "Training log: 11 epoch (41088 / 60000 train. data). Loss: 0.022709382697939873\n",
      "Training log: 11 epoch (42368 / 60000 train. data). Loss: 0.031039446592330933\n",
      "Training log: 11 epoch (43648 / 60000 train. data). Loss: 0.019857216626405716\n",
      "Training log: 11 epoch (44928 / 60000 train. data). Loss: 0.019723232835531235\n",
      "Training log: 11 epoch (46208 / 60000 train. data). Loss: 0.09184400737285614\n",
      "Training log: 11 epoch (47488 / 60000 train. data). Loss: 0.035295918583869934\n",
      "Training log: 11 epoch (48768 / 60000 train. data). Loss: 0.08649235218763351\n",
      "Training log: 11 epoch (50048 / 60000 train. data). Loss: 0.031131112948060036\n",
      "Training log: 11 epoch (51328 / 60000 train. data). Loss: 0.03270673751831055\n",
      "Training log: 11 epoch (52608 / 60000 train. data). Loss: 0.028281573206186295\n",
      "Training log: 11 epoch (53888 / 60000 train. data). Loss: 0.05083747208118439\n",
      "Training log: 11 epoch (55168 / 60000 train. data). Loss: 0.017358316108584404\n",
      "Training log: 11 epoch (56448 / 60000 train. data). Loss: 0.025088021531701088\n",
      "Training log: 11 epoch (57728 / 60000 train. data). Loss: 0.040794793516397476\n",
      "Training log: 11 epoch (59008 / 60000 train. data). Loss: 0.03617718070745468\n",
      "Test loss (avg): 0.06534148772954941, Accuracy: 0.9806\n",
      "Training log: 12 epoch (128 / 60000 train. data). Loss: 0.02337273210287094\n",
      "Training log: 12 epoch (1408 / 60000 train. data). Loss: 0.023509379476308823\n",
      "Training log: 12 epoch (2688 / 60000 train. data). Loss: 0.05122377723455429\n",
      "Training log: 12 epoch (3968 / 60000 train. data). Loss: 0.013554895296692848\n",
      "Training log: 12 epoch (5248 / 60000 train. data). Loss: 0.022678440436720848\n",
      "Training log: 12 epoch (6528 / 60000 train. data). Loss: 0.017915884032845497\n",
      "Training log: 12 epoch (7808 / 60000 train. data). Loss: 0.028478186577558517\n",
      "Training log: 12 epoch (9088 / 60000 train. data). Loss: 0.02339176833629608\n",
      "Training log: 12 epoch (10368 / 60000 train. data). Loss: 0.021120885387063026\n",
      "Training log: 12 epoch (11648 / 60000 train. data). Loss: 0.01650810055434704\n",
      "Training log: 12 epoch (12928 / 60000 train. data). Loss: 0.018120048567652702\n",
      "Training log: 12 epoch (14208 / 60000 train. data). Loss: 0.01549685187637806\n",
      "Training log: 12 epoch (15488 / 60000 train. data). Loss: 0.011225362308323383\n",
      "Training log: 12 epoch (16768 / 60000 train. data). Loss: 0.04704689234495163\n",
      "Training log: 12 epoch (18048 / 60000 train. data). Loss: 0.008229823783040047\n",
      "Training log: 12 epoch (19328 / 60000 train. data). Loss: 0.05018934980034828\n",
      "Training log: 12 epoch (20608 / 60000 train. data). Loss: 0.01265911478549242\n",
      "Training log: 12 epoch (21888 / 60000 train. data). Loss: 0.01889568381011486\n",
      "Training log: 12 epoch (23168 / 60000 train. data). Loss: 0.0073339506052434444\n",
      "Training log: 12 epoch (24448 / 60000 train. data). Loss: 0.12956608831882477\n",
      "Training log: 12 epoch (25728 / 60000 train. data). Loss: 0.030970465391874313\n",
      "Training log: 12 epoch (27008 / 60000 train. data). Loss: 0.014786427840590477\n",
      "Training log: 12 epoch (28288 / 60000 train. data). Loss: 0.04077325761318207\n",
      "Training log: 12 epoch (29568 / 60000 train. data). Loss: 0.03366367146372795\n",
      "Training log: 12 epoch (30848 / 60000 train. data). Loss: 0.011222892440855503\n",
      "Training log: 12 epoch (32128 / 60000 train. data). Loss: 0.013432776555418968\n",
      "Training log: 12 epoch (33408 / 60000 train. data). Loss: 0.0341976024210453\n",
      "Training log: 12 epoch (34688 / 60000 train. data). Loss: 0.01170913502573967\n",
      "Training log: 12 epoch (35968 / 60000 train. data). Loss: 0.02694237232208252\n",
      "Training log: 12 epoch (37248 / 60000 train. data). Loss: 0.017237277701497078\n",
      "Training log: 12 epoch (38528 / 60000 train. data). Loss: 0.027951287105679512\n",
      "Training log: 12 epoch (39808 / 60000 train. data). Loss: 0.01797465793788433\n",
      "Training log: 12 epoch (41088 / 60000 train. data). Loss: 0.011023064143955708\n",
      "Training log: 12 epoch (42368 / 60000 train. data). Loss: 0.02717553824186325\n",
      "Training log: 12 epoch (43648 / 60000 train. data). Loss: 0.006604727823287249\n",
      "Training log: 12 epoch (44928 / 60000 train. data). Loss: 0.027949362993240356\n",
      "Training log: 12 epoch (46208 / 60000 train. data). Loss: 0.021649420261383057\n",
      "Training log: 12 epoch (47488 / 60000 train. data). Loss: 0.042672768235206604\n",
      "Training log: 12 epoch (48768 / 60000 train. data). Loss: 0.019623244181275368\n",
      "Training log: 12 epoch (50048 / 60000 train. data). Loss: 0.016375631093978882\n",
      "Training log: 12 epoch (51328 / 60000 train. data). Loss: 0.07341832667589188\n",
      "Training log: 12 epoch (52608 / 60000 train. data). Loss: 0.015120144933462143\n",
      "Training log: 12 epoch (53888 / 60000 train. data). Loss: 0.04271511733531952\n",
      "Training log: 12 epoch (55168 / 60000 train. data). Loss: 0.01905853860080242\n",
      "Training log: 12 epoch (56448 / 60000 train. data). Loss: 0.0302409864962101\n",
      "Training log: 12 epoch (57728 / 60000 train. data). Loss: 0.04551505669951439\n",
      "Training log: 12 epoch (59008 / 60000 train. data). Loss: 0.009837917052209377\n",
      "Test loss (avg): 0.06625288680791855, Accuracy: 0.9804\n",
      "Training log: 13 epoch (128 / 60000 train. data). Loss: 0.0047124880366027355\n",
      "Training log: 13 epoch (1408 / 60000 train. data). Loss: 0.01915372721850872\n",
      "Training log: 13 epoch (2688 / 60000 train. data). Loss: 0.003274564864113927\n",
      "Training log: 13 epoch (3968 / 60000 train. data). Loss: 0.01767217554152012\n",
      "Training log: 13 epoch (5248 / 60000 train. data). Loss: 0.017520638182759285\n",
      "Training log: 13 epoch (6528 / 60000 train. data). Loss: 0.013223466463387012\n",
      "Training log: 13 epoch (7808 / 60000 train. data). Loss: 0.02622077614068985\n",
      "Training log: 13 epoch (9088 / 60000 train. data). Loss: 0.008369415998458862\n",
      "Training log: 13 epoch (10368 / 60000 train. data). Loss: 0.01878349296748638\n",
      "Training log: 13 epoch (11648 / 60000 train. data). Loss: 0.014147555455565453\n",
      "Training log: 13 epoch (12928 / 60000 train. data). Loss: 0.00919220969080925\n",
      "Training log: 13 epoch (14208 / 60000 train. data). Loss: 0.012623618356883526\n",
      "Training log: 13 epoch (15488 / 60000 train. data). Loss: 0.023110406473279\n",
      "Training log: 13 epoch (16768 / 60000 train. data). Loss: 0.014644782058894634\n",
      "Training log: 13 epoch (18048 / 60000 train. data). Loss: 0.008240163326263428\n",
      "Training log: 13 epoch (19328 / 60000 train. data). Loss: 0.026357784867286682\n",
      "Training log: 13 epoch (20608 / 60000 train. data). Loss: 0.03011433035135269\n",
      "Training log: 13 epoch (21888 / 60000 train. data). Loss: 0.017136214300990105\n",
      "Training log: 13 epoch (23168 / 60000 train. data). Loss: 0.007887478917837143\n",
      "Training log: 13 epoch (24448 / 60000 train. data). Loss: 0.029845116659998894\n",
      "Training log: 13 epoch (25728 / 60000 train. data). Loss: 0.03509572893381119\n",
      "Training log: 13 epoch (27008 / 60000 train. data). Loss: 0.020989131182432175\n",
      "Training log: 13 epoch (28288 / 60000 train. data). Loss: 0.009570658206939697\n",
      "Training log: 13 epoch (29568 / 60000 train. data). Loss: 0.09108846634626389\n",
      "Training log: 13 epoch (30848 / 60000 train. data). Loss: 0.00663979584351182\n",
      "Training log: 13 epoch (32128 / 60000 train. data). Loss: 0.030385741963982582\n",
      "Training log: 13 epoch (33408 / 60000 train. data). Loss: 0.024239584803581238\n",
      "Training log: 13 epoch (34688 / 60000 train. data). Loss: 0.04182269796729088\n",
      "Training log: 13 epoch (35968 / 60000 train. data). Loss: 0.004149650689214468\n",
      "Training log: 13 epoch (37248 / 60000 train. data). Loss: 0.010595256462693214\n",
      "Training log: 13 epoch (38528 / 60000 train. data). Loss: 0.010262240655720234\n",
      "Training log: 13 epoch (39808 / 60000 train. data). Loss: 0.025859491899609566\n",
      "Training log: 13 epoch (41088 / 60000 train. data). Loss: 0.021862352266907692\n",
      "Training log: 13 epoch (42368 / 60000 train. data). Loss: 0.010919241234660149\n",
      "Training log: 13 epoch (43648 / 60000 train. data). Loss: 0.01099230907857418\n",
      "Training log: 13 epoch (44928 / 60000 train. data). Loss: 0.05574818700551987\n",
      "Training log: 13 epoch (46208 / 60000 train. data). Loss: 0.01730671152472496\n",
      "Training log: 13 epoch (47488 / 60000 train. data). Loss: 0.016440045088529587\n",
      "Training log: 13 epoch (48768 / 60000 train. data). Loss: 0.024677928537130356\n",
      "Training log: 13 epoch (50048 / 60000 train. data). Loss: 0.0075016990303993225\n",
      "Training log: 13 epoch (51328 / 60000 train. data). Loss: 0.01859291084110737\n",
      "Training log: 13 epoch (52608 / 60000 train. data). Loss: 0.029464008286595345\n",
      "Training log: 13 epoch (53888 / 60000 train. data). Loss: 0.042835135012865067\n",
      "Training log: 13 epoch (55168 / 60000 train. data). Loss: 0.05661240592598915\n",
      "Training log: 13 epoch (56448 / 60000 train. data). Loss: 0.021266557276248932\n",
      "Training log: 13 epoch (57728 / 60000 train. data). Loss: 0.00594964437186718\n",
      "Training log: 13 epoch (59008 / 60000 train. data). Loss: 0.046803686767816544\n",
      "Test loss (avg): 0.07160233024358749, Accuracy: 0.9781\n",
      "Training log: 14 epoch (128 / 60000 train. data). Loss: 0.009872657246887684\n",
      "Training log: 14 epoch (1408 / 60000 train. data). Loss: 0.010114552453160286\n",
      "Training log: 14 epoch (2688 / 60000 train. data). Loss: 0.02701430395245552\n",
      "Training log: 14 epoch (3968 / 60000 train. data). Loss: 0.017699727788567543\n",
      "Training log: 14 epoch (5248 / 60000 train. data). Loss: 0.023665547370910645\n",
      "Training log: 14 epoch (6528 / 60000 train. data). Loss: 0.007174205034971237\n",
      "Training log: 14 epoch (7808 / 60000 train. data). Loss: 0.009448555298149586\n",
      "Training log: 14 epoch (9088 / 60000 train. data). Loss: 0.01582416519522667\n",
      "Training log: 14 epoch (10368 / 60000 train. data). Loss: 0.032684896141290665\n",
      "Training log: 14 epoch (11648 / 60000 train. data). Loss: 0.012934766709804535\n",
      "Training log: 14 epoch (12928 / 60000 train. data). Loss: 0.008804729208350182\n",
      "Training log: 14 epoch (14208 / 60000 train. data). Loss: 0.024767572060227394\n",
      "Training log: 14 epoch (15488 / 60000 train. data). Loss: 0.0037074165884405375\n",
      "Training log: 14 epoch (16768 / 60000 train. data). Loss: 0.03807904198765755\n",
      "Training log: 14 epoch (18048 / 60000 train. data). Loss: 0.028325382620096207\n",
      "Training log: 14 epoch (19328 / 60000 train. data). Loss: 0.015045477077364922\n",
      "Training log: 14 epoch (20608 / 60000 train. data). Loss: 0.04293376952409744\n",
      "Training log: 14 epoch (21888 / 60000 train. data). Loss: 0.013551522046327591\n",
      "Training log: 14 epoch (23168 / 60000 train. data). Loss: 0.014737039804458618\n",
      "Training log: 14 epoch (24448 / 60000 train. data). Loss: 0.014267856255173683\n",
      "Training log: 14 epoch (25728 / 60000 train. data). Loss: 0.014277724549174309\n",
      "Training log: 14 epoch (27008 / 60000 train. data). Loss: 0.01049926970154047\n",
      "Training log: 14 epoch (28288 / 60000 train. data). Loss: 0.02403300069272518\n",
      "Training log: 14 epoch (29568 / 60000 train. data). Loss: 0.014381792396306992\n",
      "Training log: 14 epoch (30848 / 60000 train. data). Loss: 0.011852080933749676\n",
      "Training log: 14 epoch (32128 / 60000 train. data). Loss: 0.008595704101026058\n",
      "Training log: 14 epoch (33408 / 60000 train. data). Loss: 0.02670353837311268\n",
      "Training log: 14 epoch (34688 / 60000 train. data). Loss: 0.02793954312801361\n",
      "Training log: 14 epoch (35968 / 60000 train. data). Loss: 0.01133775431662798\n",
      "Training log: 14 epoch (37248 / 60000 train. data). Loss: 0.018915485590696335\n",
      "Training log: 14 epoch (38528 / 60000 train. data). Loss: 0.01941749081015587\n",
      "Training log: 14 epoch (39808 / 60000 train. data). Loss: 0.015615514479577541\n",
      "Training log: 14 epoch (41088 / 60000 train. data). Loss: 0.009064593352377415\n",
      "Training log: 14 epoch (42368 / 60000 train. data). Loss: 0.030883071944117546\n",
      "Training log: 14 epoch (43648 / 60000 train. data). Loss: 0.017454786226153374\n",
      "Training log: 14 epoch (44928 / 60000 train. data). Loss: 0.012207716703414917\n",
      "Training log: 14 epoch (46208 / 60000 train. data). Loss: 0.013345800340175629\n",
      "Training log: 14 epoch (47488 / 60000 train. data). Loss: 0.01646461896598339\n",
      "Training log: 14 epoch (48768 / 60000 train. data). Loss: 0.013575111515820026\n",
      "Training log: 14 epoch (50048 / 60000 train. data). Loss: 0.01496757473796606\n",
      "Training log: 14 epoch (51328 / 60000 train. data). Loss: 0.020412161946296692\n",
      "Training log: 14 epoch (52608 / 60000 train. data). Loss: 0.016473421826958656\n",
      "Training log: 14 epoch (53888 / 60000 train. data). Loss: 0.008861656300723553\n",
      "Training log: 14 epoch (55168 / 60000 train. data). Loss: 0.009404336102306843\n",
      "Training log: 14 epoch (56448 / 60000 train. data). Loss: 0.014109144918620586\n",
      "Training log: 14 epoch (57728 / 60000 train. data). Loss: 0.021847495809197426\n",
      "Training log: 14 epoch (59008 / 60000 train. data). Loss: 0.014112988486886024\n",
      "Test loss (avg): 0.06262367182374, Accuracy: 0.9811\n",
      "Training log: 15 epoch (128 / 60000 train. data). Loss: 0.016421768814325333\n",
      "Training log: 15 epoch (1408 / 60000 train. data). Loss: 0.00947529450058937\n",
      "Training log: 15 epoch (2688 / 60000 train. data). Loss: 0.007014750503003597\n",
      "Training log: 15 epoch (3968 / 60000 train. data). Loss: 0.035086967051029205\n",
      "Training log: 15 epoch (5248 / 60000 train. data). Loss: 0.009252690710127354\n",
      "Training log: 15 epoch (6528 / 60000 train. data). Loss: 0.010577965527772903\n",
      "Training log: 15 epoch (7808 / 60000 train. data). Loss: 0.015841444954276085\n",
      "Training log: 15 epoch (9088 / 60000 train. data). Loss: 0.0028087582904845476\n",
      "Training log: 15 epoch (10368 / 60000 train. data). Loss: 0.011966586112976074\n",
      "Training log: 15 epoch (11648 / 60000 train. data). Loss: 0.015424555167555809\n",
      "Training log: 15 epoch (12928 / 60000 train. data). Loss: 0.01677253469824791\n",
      "Training log: 15 epoch (14208 / 60000 train. data). Loss: 0.00855337269604206\n",
      "Training log: 15 epoch (15488 / 60000 train. data). Loss: 0.017427463084459305\n",
      "Training log: 15 epoch (16768 / 60000 train. data). Loss: 0.008700606413185596\n",
      "Training log: 15 epoch (18048 / 60000 train. data). Loss: 0.011079728603363037\n",
      "Training log: 15 epoch (19328 / 60000 train. data). Loss: 0.03157702460885048\n",
      "Training log: 15 epoch (20608 / 60000 train. data). Loss: 0.016787327826023102\n",
      "Training log: 15 epoch (21888 / 60000 train. data). Loss: 0.004565849434584379\n",
      "Training log: 15 epoch (23168 / 60000 train. data). Loss: 0.010585554875433445\n",
      "Training log: 15 epoch (24448 / 60000 train. data). Loss: 0.019498629495501518\n",
      "Training log: 15 epoch (25728 / 60000 train. data). Loss: 0.011246023699641228\n",
      "Training log: 15 epoch (27008 / 60000 train. data). Loss: 0.023154277354478836\n",
      "Training log: 15 epoch (28288 / 60000 train. data). Loss: 0.05299964174628258\n",
      "Training log: 15 epoch (29568 / 60000 train. data). Loss: 0.0103404950350523\n",
      "Training log: 15 epoch (30848 / 60000 train. data). Loss: 0.020023617893457413\n",
      "Training log: 15 epoch (32128 / 60000 train. data). Loss: 0.026666317135095596\n",
      "Training log: 15 epoch (33408 / 60000 train. data). Loss: 0.009220828302204609\n",
      "Training log: 15 epoch (34688 / 60000 train. data). Loss: 0.014413272961974144\n",
      "Training log: 15 epoch (35968 / 60000 train. data). Loss: 0.010301937349140644\n",
      "Training log: 15 epoch (37248 / 60000 train. data). Loss: 0.014777828939259052\n",
      "Training log: 15 epoch (38528 / 60000 train. data). Loss: 0.012893366627395153\n",
      "Training log: 15 epoch (39808 / 60000 train. data). Loss: 0.004948457237333059\n",
      "Training log: 15 epoch (41088 / 60000 train. data). Loss: 0.014796985313296318\n",
      "Training log: 15 epoch (42368 / 60000 train. data). Loss: 0.014392863027751446\n",
      "Training log: 15 epoch (43648 / 60000 train. data). Loss: 0.005506507586687803\n",
      "Training log: 15 epoch (44928 / 60000 train. data). Loss: 0.006948416121304035\n",
      "Training log: 15 epoch (46208 / 60000 train. data). Loss: 0.007357217837125063\n",
      "Training log: 15 epoch (47488 / 60000 train. data). Loss: 0.005175629165023565\n",
      "Training log: 15 epoch (48768 / 60000 train. data). Loss: 0.015621636062860489\n",
      "Training log: 15 epoch (50048 / 60000 train. data). Loss: 0.0071408567018806934\n",
      "Training log: 15 epoch (51328 / 60000 train. data). Loss: 0.012430702336132526\n",
      "Training log: 15 epoch (52608 / 60000 train. data). Loss: 0.009468114003539085\n",
      "Training log: 15 epoch (53888 / 60000 train. data). Loss: 0.012739041820168495\n",
      "Training log: 15 epoch (55168 / 60000 train. data). Loss: 0.008676540106534958\n",
      "Training log: 15 epoch (56448 / 60000 train. data). Loss: 0.005724205169826746\n",
      "Training log: 15 epoch (57728 / 60000 train. data). Loss: 0.006497170310467482\n",
      "Training log: 15 epoch (59008 / 60000 train. data). Loss: 0.006425493396818638\n",
      "Test loss (avg): 0.06380651268521324, Accuracy: 0.9812\n",
      "Training log: 16 epoch (128 / 60000 train. data). Loss: 0.006145609077066183\n",
      "Training log: 16 epoch (1408 / 60000 train. data). Loss: 0.008598607033491135\n",
      "Training log: 16 epoch (2688 / 60000 train. data). Loss: 0.016100332140922546\n",
      "Training log: 16 epoch (3968 / 60000 train. data). Loss: 0.02491050399839878\n",
      "Training log: 16 epoch (5248 / 60000 train. data). Loss: 0.003768808674067259\n",
      "Training log: 16 epoch (6528 / 60000 train. data). Loss: 0.013869413174688816\n",
      "Training log: 16 epoch (7808 / 60000 train. data). Loss: 0.016943732276558876\n",
      "Training log: 16 epoch (9088 / 60000 train. data). Loss: 0.01254067849367857\n",
      "Training log: 16 epoch (10368 / 60000 train. data). Loss: 0.009751025587320328\n",
      "Training log: 16 epoch (11648 / 60000 train. data). Loss: 0.008594567887485027\n",
      "Training log: 16 epoch (12928 / 60000 train. data). Loss: 0.03973042219877243\n",
      "Training log: 16 epoch (14208 / 60000 train. data). Loss: 0.00843083206564188\n",
      "Training log: 16 epoch (15488 / 60000 train. data). Loss: 0.011603827588260174\n",
      "Training log: 16 epoch (16768 / 60000 train. data). Loss: 0.0023706993088126183\n",
      "Training log: 16 epoch (18048 / 60000 train. data). Loss: 0.009381663054227829\n",
      "Training log: 16 epoch (19328 / 60000 train. data). Loss: 0.008760705590248108\n",
      "Training log: 16 epoch (20608 / 60000 train. data). Loss: 0.016653789207339287\n",
      "Training log: 16 epoch (21888 / 60000 train. data). Loss: 0.012809386476874352\n",
      "Training log: 16 epoch (23168 / 60000 train. data). Loss: 0.010780547745525837\n",
      "Training log: 16 epoch (24448 / 60000 train. data). Loss: 0.011049474589526653\n",
      "Training log: 16 epoch (25728 / 60000 train. data). Loss: 0.004913886543363333\n",
      "Training log: 16 epoch (27008 / 60000 train. data). Loss: 0.014590359292924404\n",
      "Training log: 16 epoch (28288 / 60000 train. data). Loss: 0.010013346560299397\n",
      "Training log: 16 epoch (29568 / 60000 train. data). Loss: 0.003998911008238792\n",
      "Training log: 16 epoch (30848 / 60000 train. data). Loss: 0.010444733314216137\n",
      "Training log: 16 epoch (32128 / 60000 train. data). Loss: 0.07066114991903305\n",
      "Training log: 16 epoch (33408 / 60000 train. data). Loss: 0.003674862440675497\n",
      "Training log: 16 epoch (34688 / 60000 train. data). Loss: 0.0031317672692239285\n",
      "Training log: 16 epoch (35968 / 60000 train. data). Loss: 0.008385155349969864\n",
      "Training log: 16 epoch (37248 / 60000 train. data). Loss: 0.005616099573671818\n",
      "Training log: 16 epoch (38528 / 60000 train. data). Loss: 0.011223630048334599\n",
      "Training log: 16 epoch (39808 / 60000 train. data). Loss: 0.007772942539304495\n",
      "Training log: 16 epoch (41088 / 60000 train. data). Loss: 0.04029881954193115\n",
      "Training log: 16 epoch (42368 / 60000 train. data). Loss: 0.04008548706769943\n",
      "Training log: 16 epoch (43648 / 60000 train. data). Loss: 0.00951001513749361\n",
      "Training log: 16 epoch (44928 / 60000 train. data). Loss: 0.006650691386312246\n",
      "Training log: 16 epoch (46208 / 60000 train. data). Loss: 0.007787804584950209\n",
      "Training log: 16 epoch (47488 / 60000 train. data). Loss: 0.015280057676136494\n",
      "Training log: 16 epoch (48768 / 60000 train. data). Loss: 0.005850262474268675\n",
      "Training log: 16 epoch (50048 / 60000 train. data). Loss: 0.004425394348800182\n",
      "Training log: 16 epoch (51328 / 60000 train. data). Loss: 0.03458261117339134\n",
      "Training log: 16 epoch (52608 / 60000 train. data). Loss: 0.0021625307854264975\n",
      "Training log: 16 epoch (53888 / 60000 train. data). Loss: 0.019477032124996185\n",
      "Training log: 16 epoch (55168 / 60000 train. data). Loss: 0.006304164417088032\n",
      "Training log: 16 epoch (56448 / 60000 train. data). Loss: 0.006899812258780003\n",
      "Training log: 16 epoch (57728 / 60000 train. data). Loss: 0.006570262368768454\n",
      "Training log: 16 epoch (59008 / 60000 train. data). Loss: 0.03091772273182869\n",
      "Test loss (avg): 0.061639896076917645, Accuracy: 0.9815\n",
      "Training log: 17 epoch (128 / 60000 train. data). Loss: 0.007362122181802988\n",
      "Training log: 17 epoch (1408 / 60000 train. data). Loss: 0.013280860148370266\n",
      "Training log: 17 epoch (2688 / 60000 train. data). Loss: 0.03968708589673042\n",
      "Training log: 17 epoch (3968 / 60000 train. data). Loss: 0.011185307055711746\n",
      "Training log: 17 epoch (5248 / 60000 train. data). Loss: 0.01680389791727066\n",
      "Training log: 17 epoch (6528 / 60000 train. data). Loss: 0.005584089085459709\n",
      "Training log: 17 epoch (7808 / 60000 train. data). Loss: 0.006568488199263811\n",
      "Training log: 17 epoch (9088 / 60000 train. data). Loss: 0.008484812453389168\n",
      "Training log: 17 epoch (10368 / 60000 train. data). Loss: 0.0034063200000673532\n",
      "Training log: 17 epoch (11648 / 60000 train. data). Loss: 0.0022449649404734373\n",
      "Training log: 17 epoch (12928 / 60000 train. data). Loss: 0.02510809898376465\n",
      "Training log: 17 epoch (14208 / 60000 train. data). Loss: 0.01254566665738821\n",
      "Training log: 17 epoch (15488 / 60000 train. data). Loss: 0.01376474741846323\n",
      "Training log: 17 epoch (16768 / 60000 train. data). Loss: 0.006460063625127077\n",
      "Training log: 17 epoch (18048 / 60000 train. data). Loss: 0.003775503020733595\n",
      "Training log: 17 epoch (19328 / 60000 train. data). Loss: 0.007176772691309452\n",
      "Training log: 17 epoch (20608 / 60000 train. data). Loss: 0.005625396966934204\n",
      "Training log: 17 epoch (21888 / 60000 train. data). Loss: 0.01108961459249258\n",
      "Training log: 17 epoch (23168 / 60000 train. data). Loss: 0.0053811389952898026\n",
      "Training log: 17 epoch (24448 / 60000 train. data). Loss: 0.00760787446051836\n",
      "Training log: 17 epoch (25728 / 60000 train. data). Loss: 0.01036726776510477\n",
      "Training log: 17 epoch (27008 / 60000 train. data). Loss: 0.004484539385885\n",
      "Training log: 17 epoch (28288 / 60000 train. data). Loss: 0.006597005762159824\n",
      "Training log: 17 epoch (29568 / 60000 train. data). Loss: 0.001758964266628027\n",
      "Training log: 17 epoch (30848 / 60000 train. data). Loss: 0.007474323268979788\n",
      "Training log: 17 epoch (32128 / 60000 train. data). Loss: 0.01578844152390957\n",
      "Training log: 17 epoch (33408 / 60000 train. data). Loss: 0.005891835782676935\n",
      "Training log: 17 epoch (34688 / 60000 train. data). Loss: 0.023147400468587875\n",
      "Training log: 17 epoch (35968 / 60000 train. data). Loss: 0.008476367220282555\n",
      "Training log: 17 epoch (37248 / 60000 train. data). Loss: 0.008344755508005619\n",
      "Training log: 17 epoch (38528 / 60000 train. data). Loss: 0.022981960326433182\n",
      "Training log: 17 epoch (39808 / 60000 train. data). Loss: 0.017152249813079834\n",
      "Training log: 17 epoch (41088 / 60000 train. data). Loss: 0.019124582409858704\n",
      "Training log: 17 epoch (42368 / 60000 train. data). Loss: 0.005685607437044382\n",
      "Training log: 17 epoch (43648 / 60000 train. data). Loss: 0.005091523751616478\n",
      "Training log: 17 epoch (44928 / 60000 train. data). Loss: 0.004470133688300848\n",
      "Training log: 17 epoch (46208 / 60000 train. data). Loss: 0.013300220482051373\n",
      "Training log: 17 epoch (47488 / 60000 train. data). Loss: 0.00839152093976736\n",
      "Training log: 17 epoch (48768 / 60000 train. data). Loss: 0.009480231441557407\n",
      "Training log: 17 epoch (50048 / 60000 train. data). Loss: 0.02756511978805065\n",
      "Training log: 17 epoch (51328 / 60000 train. data). Loss: 0.008114396594464779\n",
      "Training log: 17 epoch (52608 / 60000 train. data). Loss: 0.0022877673618495464\n",
      "Training log: 17 epoch (53888 / 60000 train. data). Loss: 0.002065257867798209\n",
      "Training log: 17 epoch (55168 / 60000 train. data). Loss: 0.04257709160447121\n",
      "Training log: 17 epoch (56448 / 60000 train. data). Loss: 0.014397748745977879\n",
      "Training log: 17 epoch (57728 / 60000 train. data). Loss: 0.015709182247519493\n",
      "Training log: 17 epoch (59008 / 60000 train. data). Loss: 0.005035859066992998\n",
      "Test loss (avg): 0.06732481764853, Accuracy: 0.9815\n",
      "Training log: 18 epoch (128 / 60000 train. data). Loss: 0.014555813744664192\n",
      "Training log: 18 epoch (1408 / 60000 train. data). Loss: 0.001686452655121684\n",
      "Training log: 18 epoch (2688 / 60000 train. data). Loss: 0.0071337539702653885\n",
      "Training log: 18 epoch (3968 / 60000 train. data). Loss: 0.026966126635670662\n",
      "Training log: 18 epoch (5248 / 60000 train. data). Loss: 0.0015291463350877166\n",
      "Training log: 18 epoch (6528 / 60000 train. data). Loss: 0.008256250061094761\n",
      "Training log: 18 epoch (7808 / 60000 train. data). Loss: 0.02329329401254654\n",
      "Training log: 18 epoch (9088 / 60000 train. data). Loss: 0.00476273475214839\n",
      "Training log: 18 epoch (10368 / 60000 train. data). Loss: 0.010966338217258453\n",
      "Training log: 18 epoch (11648 / 60000 train. data). Loss: 0.004142201039940119\n",
      "Training log: 18 epoch (12928 / 60000 train. data). Loss: 0.002635915530845523\n",
      "Training log: 18 epoch (14208 / 60000 train. data). Loss: 0.00802650023251772\n",
      "Training log: 18 epoch (15488 / 60000 train. data). Loss: 0.0051101925782859325\n",
      "Training log: 18 epoch (16768 / 60000 train. data). Loss: 0.003161391708999872\n",
      "Training log: 18 epoch (18048 / 60000 train. data). Loss: 0.009080245159566402\n",
      "Training log: 18 epoch (19328 / 60000 train. data). Loss: 0.007767296861857176\n",
      "Training log: 18 epoch (20608 / 60000 train. data). Loss: 0.018596939742565155\n",
      "Training log: 18 epoch (21888 / 60000 train. data). Loss: 0.0020389591809362173\n",
      "Training log: 18 epoch (23168 / 60000 train. data). Loss: 0.010895944200456142\n",
      "Training log: 18 epoch (24448 / 60000 train. data). Loss: 0.0068266852758824825\n",
      "Training log: 18 epoch (25728 / 60000 train. data). Loss: 0.012473778799176216\n",
      "Training log: 18 epoch (27008 / 60000 train. data). Loss: 0.004889874719083309\n",
      "Training log: 18 epoch (28288 / 60000 train. data). Loss: 0.006847875192761421\n",
      "Training log: 18 epoch (29568 / 60000 train. data). Loss: 0.005649619270116091\n",
      "Training log: 18 epoch (30848 / 60000 train. data). Loss: 0.004616583231836557\n",
      "Training log: 18 epoch (32128 / 60000 train. data). Loss: 0.003217813093215227\n",
      "Training log: 18 epoch (33408 / 60000 train. data). Loss: 0.0016407509101554751\n",
      "Training log: 18 epoch (34688 / 60000 train. data). Loss: 0.027494415640830994\n",
      "Training log: 18 epoch (35968 / 60000 train. data). Loss: 0.007897831499576569\n",
      "Training log: 18 epoch (37248 / 60000 train. data). Loss: 0.033089473843574524\n",
      "Training log: 18 epoch (38528 / 60000 train. data). Loss: 0.00849877018481493\n",
      "Training log: 18 epoch (39808 / 60000 train. data). Loss: 0.00395957799628377\n",
      "Training log: 18 epoch (41088 / 60000 train. data). Loss: 0.0204241331666708\n",
      "Training log: 18 epoch (42368 / 60000 train. data). Loss: 0.01259622536599636\n",
      "Training log: 18 epoch (43648 / 60000 train. data). Loss: 0.001968527678400278\n",
      "Training log: 18 epoch (44928 / 60000 train. data). Loss: 0.0033474303781986237\n",
      "Training log: 18 epoch (46208 / 60000 train. data). Loss: 0.0029312660917639732\n",
      "Training log: 18 epoch (47488 / 60000 train. data). Loss: 0.02077244408428669\n",
      "Training log: 18 epoch (48768 / 60000 train. data). Loss: 0.008453522808849812\n",
      "Training log: 18 epoch (50048 / 60000 train. data). Loss: 0.0038815520238131285\n",
      "Training log: 18 epoch (51328 / 60000 train. data). Loss: 0.0039319307543337345\n",
      "Training log: 18 epoch (52608 / 60000 train. data). Loss: 0.005075280088931322\n",
      "Training log: 18 epoch (53888 / 60000 train. data). Loss: 0.003254291135817766\n",
      "Training log: 18 epoch (55168 / 60000 train. data). Loss: 0.020331917330622673\n",
      "Training log: 18 epoch (56448 / 60000 train. data). Loss: 0.014631099067628384\n",
      "Training log: 18 epoch (57728 / 60000 train. data). Loss: 0.007645467296242714\n",
      "Training log: 18 epoch (59008 / 60000 train. data). Loss: 0.013753814622759819\n",
      "Test loss (avg): 0.06227220598459244, Accuracy: 0.9828\n",
      "Training log: 19 epoch (128 / 60000 train. data). Loss: 0.0021093261893838644\n",
      "Training log: 19 epoch (1408 / 60000 train. data). Loss: 0.01604883372783661\n",
      "Training log: 19 epoch (2688 / 60000 train. data). Loss: 0.005198993720114231\n",
      "Training log: 19 epoch (3968 / 60000 train. data). Loss: 0.0027875558007508516\n",
      "Training log: 19 epoch (5248 / 60000 train. data). Loss: 0.014595787040889263\n",
      "Training log: 19 epoch (6528 / 60000 train. data). Loss: 0.0058698831126093864\n",
      "Training log: 19 epoch (7808 / 60000 train. data). Loss: 0.003317600814625621\n",
      "Training log: 19 epoch (9088 / 60000 train. data). Loss: 0.0029236709233373404\n",
      "Training log: 19 epoch (10368 / 60000 train. data). Loss: 0.013799444772303104\n",
      "Training log: 19 epoch (11648 / 60000 train. data). Loss: 0.002691882662475109\n",
      "Training log: 19 epoch (12928 / 60000 train. data). Loss: 0.004172939341515303\n",
      "Training log: 19 epoch (14208 / 60000 train. data). Loss: 0.006711910478770733\n",
      "Training log: 19 epoch (15488 / 60000 train. data). Loss: 0.0015357598895207047\n",
      "Training log: 19 epoch (16768 / 60000 train. data). Loss: 0.005042345263063908\n",
      "Training log: 19 epoch (18048 / 60000 train. data). Loss: 0.013431037776172161\n",
      "Training log: 19 epoch (19328 / 60000 train. data). Loss: 0.0014267561491578817\n",
      "Training log: 19 epoch (20608 / 60000 train. data). Loss: 0.002340034581720829\n",
      "Training log: 19 epoch (21888 / 60000 train. data). Loss: 0.009340035729110241\n",
      "Training log: 19 epoch (23168 / 60000 train. data). Loss: 0.028294263407588005\n",
      "Training log: 19 epoch (24448 / 60000 train. data). Loss: 0.014105605892837048\n",
      "Training log: 19 epoch (25728 / 60000 train. data). Loss: 0.008553375490009785\n",
      "Training log: 19 epoch (27008 / 60000 train. data). Loss: 0.002030672039836645\n",
      "Training log: 19 epoch (28288 / 60000 train. data). Loss: 0.012822444550693035\n",
      "Training log: 19 epoch (29568 / 60000 train. data). Loss: 0.0038226062897592783\n",
      "Training log: 19 epoch (30848 / 60000 train. data). Loss: 0.007001963444054127\n",
      "Training log: 19 epoch (32128 / 60000 train. data). Loss: 0.01227352861315012\n",
      "Training log: 19 epoch (33408 / 60000 train. data). Loss: 0.003940642345696688\n",
      "Training log: 19 epoch (34688 / 60000 train. data). Loss: 0.0027132725808769464\n",
      "Training log: 19 epoch (35968 / 60000 train. data). Loss: 0.005830681882798672\n",
      "Training log: 19 epoch (37248 / 60000 train. data). Loss: 0.000716598064173013\n",
      "Training log: 19 epoch (38528 / 60000 train. data). Loss: 0.0025254159700125456\n",
      "Training log: 19 epoch (39808 / 60000 train. data). Loss: 0.0021755483467131853\n",
      "Training log: 19 epoch (41088 / 60000 train. data). Loss: 0.0021144330967217684\n",
      "Training log: 19 epoch (42368 / 60000 train. data). Loss: 0.008839597925543785\n",
      "Training log: 19 epoch (43648 / 60000 train. data). Loss: 0.009987717494368553\n",
      "Training log: 19 epoch (44928 / 60000 train. data). Loss: 0.01998083107173443\n",
      "Training log: 19 epoch (46208 / 60000 train. data). Loss: 0.0019163214601576328\n",
      "Training log: 19 epoch (47488 / 60000 train. data). Loss: 0.0038884636014699936\n",
      "Training log: 19 epoch (48768 / 60000 train. data). Loss: 0.0007748287753202021\n",
      "Training log: 19 epoch (50048 / 60000 train. data). Loss: 0.0029214101377874613\n",
      "Training log: 19 epoch (51328 / 60000 train. data). Loss: 0.003505604574456811\n",
      "Training log: 19 epoch (52608 / 60000 train. data). Loss: 0.0036894946824759245\n",
      "Training log: 19 epoch (53888 / 60000 train. data). Loss: 0.003649716032668948\n",
      "Training log: 19 epoch (55168 / 60000 train. data). Loss: 0.002998084295541048\n",
      "Training log: 19 epoch (56448 / 60000 train. data). Loss: 0.003499394515529275\n",
      "Training log: 19 epoch (57728 / 60000 train. data). Loss: 0.0047057862393558025\n",
      "Training log: 19 epoch (59008 / 60000 train. data). Loss: 0.0029334884602576494\n",
      "Test loss (avg): 0.06601522578955628, Accuracy: 0.9816\n",
      "Training log: 20 epoch (128 / 60000 train. data). Loss: 0.007085584104061127\n",
      "Training log: 20 epoch (1408 / 60000 train. data). Loss: 0.010620221495628357\n",
      "Training log: 20 epoch (2688 / 60000 train. data). Loss: 0.001725378562696278\n",
      "Training log: 20 epoch (3968 / 60000 train. data). Loss: 0.0018877830589190125\n",
      "Training log: 20 epoch (5248 / 60000 train. data). Loss: 0.005403932649642229\n",
      "Training log: 20 epoch (6528 / 60000 train. data). Loss: 0.004322233609855175\n",
      "Training log: 20 epoch (7808 / 60000 train. data). Loss: 0.002931778086349368\n",
      "Training log: 20 epoch (9088 / 60000 train. data). Loss: 0.002971832873299718\n",
      "Training log: 20 epoch (10368 / 60000 train. data). Loss: 0.003065338358283043\n",
      "Training log: 20 epoch (11648 / 60000 train. data). Loss: 0.0019908174872398376\n",
      "Training log: 20 epoch (12928 / 60000 train. data). Loss: 0.001856932882219553\n",
      "Training log: 20 epoch (14208 / 60000 train. data). Loss: 0.002496907487511635\n",
      "Training log: 20 epoch (15488 / 60000 train. data). Loss: 0.006434702314436436\n",
      "Training log: 20 epoch (16768 / 60000 train. data). Loss: 0.023733917623758316\n",
      "Training log: 20 epoch (18048 / 60000 train. data). Loss: 0.00338862300850451\n",
      "Training log: 20 epoch (19328 / 60000 train. data). Loss: 0.009198488667607307\n",
      "Training log: 20 epoch (20608 / 60000 train. data). Loss: 0.012862294912338257\n",
      "Training log: 20 epoch (21888 / 60000 train. data). Loss: 0.0032698095310479403\n",
      "Training log: 20 epoch (23168 / 60000 train. data). Loss: 0.0030099800787866116\n",
      "Training log: 20 epoch (24448 / 60000 train. data). Loss: 0.008505262434482574\n",
      "Training log: 20 epoch (25728 / 60000 train. data). Loss: 0.0026632833760231733\n",
      "Training log: 20 epoch (27008 / 60000 train. data). Loss: 0.002447479171678424\n",
      "Training log: 20 epoch (28288 / 60000 train. data). Loss: 0.0015399320982396603\n",
      "Training log: 20 epoch (29568 / 60000 train. data). Loss: 0.00265753292478621\n",
      "Training log: 20 epoch (30848 / 60000 train. data). Loss: 0.0019910710398107767\n",
      "Training log: 20 epoch (32128 / 60000 train. data). Loss: 0.0010573908220976591\n",
      "Training log: 20 epoch (33408 / 60000 train. data). Loss: 0.005310786422342062\n",
      "Training log: 20 epoch (34688 / 60000 train. data). Loss: 0.0006218687049113214\n",
      "Training log: 20 epoch (35968 / 60000 train. data). Loss: 0.004653405863791704\n",
      "Training log: 20 epoch (37248 / 60000 train. data). Loss: 0.0016245143488049507\n",
      "Training log: 20 epoch (38528 / 60000 train. data). Loss: 0.006235226057469845\n",
      "Training log: 20 epoch (39808 / 60000 train. data). Loss: 0.012020451948046684\n",
      "Training log: 20 epoch (41088 / 60000 train. data). Loss: 0.006932115647941828\n",
      "Training log: 20 epoch (42368 / 60000 train. data). Loss: 0.004725771956145763\n",
      "Training log: 20 epoch (43648 / 60000 train. data). Loss: 0.00315967109054327\n",
      "Training log: 20 epoch (44928 / 60000 train. data). Loss: 0.0027325081173330545\n",
      "Training log: 20 epoch (46208 / 60000 train. data). Loss: 0.00394772132858634\n",
      "Training log: 20 epoch (47488 / 60000 train. data). Loss: 0.0025783777236938477\n",
      "Training log: 20 epoch (48768 / 60000 train. data). Loss: 0.0031167427077889442\n",
      "Training log: 20 epoch (50048 / 60000 train. data). Loss: 0.00923212617635727\n",
      "Training log: 20 epoch (51328 / 60000 train. data). Loss: 0.0015745667042210698\n",
      "Training log: 20 epoch (52608 / 60000 train. data). Loss: 0.007944527082145214\n",
      "Training log: 20 epoch (53888 / 60000 train. data). Loss: 0.005658363923430443\n",
      "Training log: 20 epoch (55168 / 60000 train. data). Loss: 0.01560258585959673\n",
      "Training log: 20 epoch (56448 / 60000 train. data). Loss: 0.010188750922679901\n",
      "Training log: 20 epoch (57728 / 60000 train. data). Loss: 0.003469895338639617\n",
      "Training log: 20 epoch (59008 / 60000 train. data). Loss: 0.0007336320704780519\n",
      "Test loss (avg): 0.0673587070286274, Accuracy: 0.9807\n",
      "{'train_loss': [tensor(0.3020, grad_fn=<NllLossBackward>), tensor(0.3203, grad_fn=<NllLossBackward>), tensor(0.2932, grad_fn=<NllLossBackward>), tensor(0.0628, grad_fn=<NllLossBackward>), tensor(0.1429, grad_fn=<NllLossBackward>), tensor(0.0891, grad_fn=<NllLossBackward>), tensor(0.0681, grad_fn=<NllLossBackward>), tensor(0.0488, grad_fn=<NllLossBackward>), tensor(0.0195, grad_fn=<NllLossBackward>), tensor(0.0294, grad_fn=<NllLossBackward>), tensor(0.0146, grad_fn=<NllLossBackward>), tensor(0.0141, grad_fn=<NllLossBackward>), tensor(0.0279, grad_fn=<NllLossBackward>), tensor(0.0111, grad_fn=<NllLossBackward>), tensor(0.0126, grad_fn=<NllLossBackward>), tensor(0.0135, grad_fn=<NllLossBackward>), tensor(0.0043, grad_fn=<NllLossBackward>), tensor(0.0098, grad_fn=<NllLossBackward>), tensor(0.0099, grad_fn=<NllLossBackward>), tensor(0.0070, grad_fn=<NllLossBackward>)], 'test_loss': [0.25729767837524414, 0.1997483792990446, 0.15971785962581633, 0.13164567719399928, 0.10931385655403138, 0.09496908477544784, 0.0880565966129303, 0.07938116746544838, 0.07256627287864685, 0.07424206672310829, 0.06534148772954941, 0.06625288680791855, 0.07160233024358749, 0.06262367182374, 0.06380651268521324, 0.061639896076917645, 0.06732481764853, 0.06227220598459244, 0.06601522578955628, 0.0673587070286274], 'test_acc': [0.9232, 0.9389, 0.9522, 0.9613, 0.9665, 0.9716, 0.9725, 0.9768, 0.9774, 0.9759, 0.9806, 0.9804, 0.9781, 0.9811, 0.9812, 0.9815, 0.9815, 0.9828, 0.9816, 0.9807]}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "PyTorchでMNISTを学習させる\n",
    " \n",
    ":summary   PyTorchで単純な多層パーセプトロンを構築してみる\n",
    ":author    RightCode Inc. (https://rightcode.co.jp)\n",
    "\"\"\"\n",
    " \n",
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    " \n",
    "class MyNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(28*28, 1000)\n",
    "        self.fc2 = torch.nn.Linear(1000, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.fc2(x)\n",
    " \n",
    "        return f.log_softmax(x, dim=1)\n",
    " \n",
    " \n",
    "def load_MNIST(batch=128, intensity=1.0):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=True,\n",
    "                       download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data',\n",
    "                       train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x * intensity)\n",
    "                       ])),\n",
    "        batch_size=batch,\n",
    "        shuffle=True)\n",
    " \n",
    "    return {'train': train_loader, 'test': test_loader}\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    # 学習回数\n",
    "    epoch = 20\n",
    " \n",
    "    # 学習結果の保存用\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'test_loss': [],\n",
    "        'test_acc': [],\n",
    "    }\n",
    " \n",
    "    # ネットワークを構築\n",
    "    net: torch.nn.Module = MyNet()\n",
    " \n",
    "    # MNISTのデータローダーを取得\n",
    "    loaders = load_MNIST()\n",
    " \n",
    "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
    " \n",
    "    for e in range(epoch):\n",
    " \n",
    "        \"\"\" Training Part\"\"\"\n",
    "        loss = None\n",
    "        # 学習開始 (再開)\n",
    "        net.train(True)  # 引数は省略可能\n",
    "        for i, (data, target) in enumerate(loaders['train']):\n",
    "            # 全結合のみのネットワークでは入力を1次元に\n",
    "            # print(data.shape)  # torch.Size([128, 1, 28, 28])\n",
    "            data = data.view(-1, 28*28)\n",
    "            # print(data.shape)  # torch.Size([128, 784])\n",
    " \n",
    "            optimizer.zero_grad()\n",
    "            output = net(data)\n",
    "            loss = f.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    " \n",
    "            if i % 10 == 0:\n",
    "                print('Training log: {} epoch ({} / 60000 train. data). Loss: {}'.format(e+1,\n",
    "                                                                                         (i+1)*128,\n",
    "                                                                                         loss.item())\n",
    "                      )\n",
    " \n",
    "        history['train_loss'].append(loss)\n",
    " \n",
    "        \"\"\" Test Part \"\"\"\n",
    "        # 学習のストップ\n",
    "        net.eval()  # または net.train(False) でも良い\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    " \n",
    "        with torch.no_grad():\n",
    "            for data, target in loaders['test']:\n",
    "                data = data.view(-1, 28 * 28)\n",
    "                output = net(data)\n",
    "                test_loss += f.nll_loss(output, target, reduction='sum').item()\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    " \n",
    "        test_loss /= 10000\n",
    " \n",
    "        print('Test loss (avg): {}, Accuracy: {}'.format(test_loss,\n",
    "                                                         correct / 10000))\n",
    " \n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_acc'].append(correct / 10000)\n",
    " \n",
    "    # 結果の出力と描画\n",
    "    print(history)\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, epoch+1), history['train_loss'], label='train_loss')\n",
    "    plt.plot(range(1, epoch+1), history['test_loss'], label='test_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend()\n",
    "    plt.savefig('loss.png')\n",
    " \n",
    "    plt.figure()\n",
    "    plt.plot(range(1, epoch+1), history['test_acc'])\n",
    "    plt.title('test accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.savefig('test_acc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
